\documentclass[nojss]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------
%\VignetteIndexEntry{sstvars: Structural Smooth Transition Vector Autoregressive Models R}

%% recommended packages
\usepackage{thumbpdf, lmodern}

%% Packages added by the author
\usepackage{amsmath} % \align, etc.
\usepackage{amssymb} % \mathbb, etc.
\usepackage{mathtools} % matrix*: [c] centered, [l] left-aligned, [r] right-aligned

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}
\newtheorem{condition}{Condition}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}


%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}



%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation (and optionally ORCID link)
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Savi Virolainen\\ University of Helsinki}
\Plainauthor{Savi Virolainen}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{sstvars: Structural Smooth Transition Vector Autoregressive Models in \proglang{R}}
\Plaintitle{sstvars: Structural Smooth Transition Vector Autoregressive Models in R}
\Shorttitle{Structural Smooth Transition Vector Autoregressive Models \proglang{R}}

%% - \Abstract{} almost as usual
\Abstract{
  We describe the \proglang{R} package \pkg{sstvars}, which provides tools for estimating and analyzing the reduced form and structural smooth transition vector autoregressive (STVAR) models. The package implements various transition weight functions, conditional distributions, identification methods, and parameter restrictions. The model parameters are estimated with the method of maximum likelihood by running multiple rounds of a two-phase estimation procedure in which a genetic algorithm is used to find starting values for a gradient based method. For evaluating the adequacy of the estimated models, \pkg{sstvars} utilizes residuals based diagnostics and provides functions for graphical diagnostics and for calculating formal diagnostic tests. \pkg{sstvars} also accommodates the estimation of linear impulse response functions, nonlinear generalized impulse response functions, and generalized forecast error variance decompositions. Further functionality includes hypothesis testing, plotting the profile log-likelihood functions about the estimate, simulation from STVAR processes, and forecasting, for example. We illustrate the use of \pkg{sstvars} with a quarterly series consisting of two U.S. variables: the percentage change of real GDP and the percentage change of GDP implicit price deflator.}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{smooth transition vector autoregressive model, structural smooth transition vector autoregressive model, regime-switching,
SVAR, STVAR, maximum likelihood estimation}
\Plainkeywords{smooth transition vector autoregressive model, structural smooth transition vector autoregressive model, regime-switching,
SVAR, STVAR, maximum likelihood estimation}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Savi Virolainen\\
  Faculty of Social Sciences\\
  University of Helsinki\\
  P. O. Box 17, FI-0014 University of Helsinki, Finland\\
  E-mail: \email{savi.virolainen@helsinki.fi}
}


\begin{document}

\section{Introduction}
Linear vector autoregressive (VAR) models are a standard tools in time series econometrics. They can be employed to answer questions about the statistical relationships of different variables or to forecast future values of the process, for example. Structural VAR model allows to trace out the effects of economic shocks that have been identified by the researcher. With an appropriate choice of the autoregressive order $p$, a linear VAR is often able to filter out autocorrelation from the series very well. If the errors are assumed to follow an autoregressive conditional heteroskedasticity (ARCH) process, the model is also often able to adequately filter out conditional heteroskedasticity from the series.

In some cases, linear VAR models are not, however, capable to capture all the relevant characteristics of the series. This includes shifts in the mean or volatility, and changes in the autoregressive dynamics of the process. Such nonlinear features frequently occur in economic time series when the underlying data generating dynamics vary in time, for example, depending the specific state of the economy.

Various types of time series models capable of capturing this kind of regime-switching behavior have been proposed, one of them is the smooth transition vector autoregressive (STVAR) models that allow to capture gradual shifts in the dynamics of the data. They consist of a finite number of regimes, each of which are linear vector autoregressions that are characterized by different autoregressive coefficients or error term covariance matrices. The package \pkg{sstvars} considers STVAR models in which, at each point of time, the observation is a weighted average of the conditional means of the regimes plus a random error whose covariance matrix is a weighted average of the covariance matrices of the regimes. The weights, in turn, are expressed in terms of time-varying transition weights that depend on the preceding observations. Different STVAR models can be created by specifying the transition weights or the error distribution in various ways. See \cite{Hubrich+Terasvirta:2013} for a survey on STVAR literature, including formulations more general than our framework.

This manuscript describes the \proglang{R} package \pkg{sstvars} providing a set of easy-to-use tools for STVAR modeling, including unconstrained and constrained maximum likelihood (ML) estimation of the model parameters, residual based model diagnostics, estimation of linear impulse response functions, nonlinear generalized impulse response functions, and generalized forecast error variance decompositions. Further functionality includes hypothesis testing, plotting the profile log-likelihood functions about the estimate, simulation from STVAR processes, and forecasting, for example. Various transition weight functions are accommodated, including logistic weights \cite{Anderson+Vahid:1998}, multinomial logit weights, exponential weights \citep[e.g.,][]{Hubrich+Terasvirta:2013}, threshold weights \citep{Tsay:1998}, and transition weights that defined as weighted relative likelihoods of the regimes corresponding to the preceding $p$ observations (FILL IN REFERENCE). Currently, the accommodated conditional distributions include Student's $t$ distribution and Gaussian distribution, whereas the accommodated identification methods include recursive identification and identification by heteroskedasticity. More conditional distributions and identification methods are probably added in the future, however.

The estimation of the model parameters can, in some cases, be rather tricky. This is because the transition weights are determined endogenously, which induces a large number of modes to the log-likelihood function and large areas of the parameter space where the log-likelihood function is flat in multiple directions. Therefore, the model parameters are estimated by running multiple rounds of a two-phase estimation procedure in which a modified genetic algorithm is used to find starting values for a gradient based variable metric algorithm. Because of the multimodality of the log-likelihood function, some of the estimation rounds may end up in different local maximum points, thereby enabling the researcher to build models not only based on the global maximum point but also on the local ones. The estimated models can be conveniently examined with the \code{summary} and \code{plot} methods.

The remainder of this paper is organized as follows. Section~\ref{sec:models} defines the implemented STVAR models and discusses some of their properties. For structural models, identification of the shocks is also covered. Section~\ref{sec:estimation} discusses estimation of the model parameters. We also illustrates how the STVAR models can be estimated and examined with \pkg{sstvars} and how various parameter restrictions can be imposed in the estimation. In Section~\ref{sec:res}, we describe demonstrate the use residual based diagnostics to evaluate model adequacy. Section~\ref{sec:impulseresponse} discusses impulse response analysis, including generalized impulse response functions and generalized forecast error variance decompositions. Section~\ref{sec:STVAR} shows how the STVAR models can be built with given parameter values. In Section~\ref{sec:simufore}, we first show how to simulate observations from a STVAR process, and then we illustrate how to forecast future values of a STVAR process with a simulation-based Monte Carlo method. Finally, Section~\ref{sec:summary} concludes and collects some useful functions in \pkg{sstvars} to a single table for convenience. Throughout this paper, we illustrate the use of \pkg{sstvars} with a quarterly series consisting of two U.S. variables: the percentage change of real GDP and the percentage change of GDP implicit price deflator, covering the period from 1959Q1 to 2019Q4.

%We deploy the notation $n_d(\boldsymbol{\mu},\boldsymbol{\Gamma})$ for the $d$-dimensional normal distribution with mean $\boldsymbol{\mu}$ and (positive definite) covariance matrix $\boldsymbol{\Gamma}$, and $t_d(\boldsymbol{\mu},\boldsymbol{\Gamma},\nu)$ for the $d$-dimensional $t$-distribution with mean $\boldsymbol{\mu}$, (positive definite) covariance matrix $\boldsymbol{\Gamma}$, and $\nu>2$ degrees of freedom. The corresponding density functions are denoted as $n_d(\cdot;\boldsymbol{\mu},\boldsymbol{\Gamma})$ and $t_d(\cdot;\boldsymbol{\mu},\boldsymbol{\Gamma},\nu)$, respectively. By $\boldsymbol{1}_p=(1,...,1)$ ($p \times 1$), we denote $p$-dimensional vector of ones. Finally, $\otimes$ denotes Kronecker product.

\section{Smooth Transition Vector Autoregressive Models}\label{sec:models}
This section describes the STVAR models implemented in \pkg{sstvars}. First, we describe the general framework of STVAR models accommodates by \pkg{sstvars} and present a sufficient condition for their ergodic stationarity. Then, we present the implemented specifications transition weight functions and conditional distributions. Finally, we discuss structural STVAR models and implemented methods for identification of the shocks.

\subsection{General framework for STVAR models}\label{sec:genstvar}
Let $y_t$, $t=1,2,...$, be the $d$-dimensional time series of interest and $\mathcal{F}_{t-1}$ denote the $\sigma$-algebra generated by the random vectors $\lbrace y_{t-j}, j>0 \rbrace$. We consider STVAR models with $M$ regimes and autoregressive order $p$ assumed to satisfy
\begin{align}
y_t &=\sum_{m=1}^M \alpha_{m,t}\mu_{m,t} + B_te_t, \quad e_{t} \sim IID(0, I_d)\label{eq:stvar1} \\
\mu_{m,t} &= \phi_{m,0} + \sum_{i=1}^{p}A_{m,i}y_{t-i}, \quad m=1,...,M,\label{eq:stvar2}\\
B_tB_t' &= \sum_{m=1}^M \alpha_{m,t}\Omega_m, \label{eq:stvar3}
\end{align}
where $\phi_{m,0}\in\mathbb{R}^{d}$ are intercept parameters, $\Omega_1,...,\Omega_M$ are the positive definite $(d\times d)$ covariance matrices of the regimes, $B_t$ is an invertible ($d\times d$) impact matrix that governs the contemporaneous relationships of the shocks and is a function of $\lbrace y_{t-j}, j=1,...,p \rbrace$. For the reduced form model, any invertible matrix $B_t$ that satisfies Equation~(\ref{eq:stvar3}) can be assumed without loss of generality, whereas structural models assume a specific structure on $B_t$. The structural errors $e_{t}$ $(d\times 1)$ are independent and identically distributed with mean zero and identity covariance matrix, and they are independent of $\mathcal{F}_{t-1}$. The transition weights $\alpha_{m,t}$ are assumed to be $\mathcal{F}_{t-1}$-measurable functions of $\lbrace y_{t-j}, j=1,...,p \rbrace$ and to satisfy $\sum_{m=1}^{M}\alpha_{m,t}=1$ at all $t$. They express the proportions of the regimes the process is on at each point of time. Through, a STVAR model with autoregressive order $p$ and $M$ regimes is referred to as a STVAR($p,M$) model, whenever the order of the model needs to be emphasized.

Conditional on $\mathcal{F}_{t-1}$, the conditional mean of the above described process is $\mu_{y,t} \equiv E[y_t|\mathcal{F}_{t-1}] = \sum_{m=1}^M \alpha_{m,t}\mu_{m,t}$, and the conditional covariance matrix is $\Omega_{y,t}\equiv \text{Cov}(y_t|\mathcal{F}_{t-1}) = \sum_{m=1}^M \alpha_{m,t}\Omega_m$. In other words, the conditional mean is a weighted sum the regime-specific means $\mu_{m,t}$ with the weights given by the transition weights $\alpha_{m,t}$, whereas the conditional covariance matrix is a weighted sum of the regime-specific conditional covariance matrices $\Omega_m$. Different STVAR models are obtained by specifying the transition weights or the error distribution (i.e., the conditional distribution) in various ways. See \cite{Hubrich+Terasvirta:2013} for a survey on STVAR literature, including formulations more general than our framework.

The package \pkg{sstvars} accommodates models in which the transition weights are functions of $\lbrace y_{t-j}, j=1,...,p \rbrace$, which is required for applicability of the stationarity condition presented below. Moreover, $\mathcal{F}_{t-1}$-measurability of the transition weights ensures that the true generalized impulse responses functions \citep{Koop+Pesaran+Potter:1996} can be easily estimated, as completely exogenous switching-variables are excluded from affecting the weights. We also assume that the transition weights are identical for all the individual equations in (\ref{eq:stvar1}) and (\ref{eq:stvar3}), which is also required for applicability of the stationarity condition. Consequently, at each $t$‚ the process can be described as a weighted sum of linear vector autoregressions.

\subsubsection{Stationarity condition}\label{sec:stationarity}
It can be shown that a sufficient condition for the ergodic stationarity of the STVAR model (\ref{eq:stvar1})-(\ref{eq:stvar3}) can expressed in therms of the joint spectral radius (JSR) of certain matrices \citep{Kheifets+Saikkonen:2020}. The JSR of a finite set of square matrices $\mathcal{A}$ is defined by
\begin{equation}
\rho(\mathcal{A}) = \underset{j\rightarrow \infty}{\limsup}\left(\underset{A\in \mathcal{A}^j}{\sup}\rho(A) \right)^{1/j},
\end{equation}
where $\mathcal{A}^j=\lbrace A_1A_2...A_j:A_i\in\mathcal{A}\rbrace$ and $\rho(A)$ is the spectral radius of the square matrix $A$.

Consider the companion form AR matrices of the regimes defined as
\begin{equation}\label{eq:boldA}
\boldsymbol{A}_m =
\underset{(dp\times dp)}{\begin{bmatrix}
A_{m,1} & A_{m,2} & \cdots & A_{m,p-1} & A_{m,p} \\
I_d  & 0     & \cdots & 0            & 0 \\
0     & I_d  &             & 0            & 0 \\
\vdots &   & \ddots & \vdots    & \vdots \\
0     & 0     & \hdots & I_d         & 0
\end{bmatrix}}, \
m=1,...,M.
\end{equation}
\citet[][Theorem 1]{Kheifets+Saikkonen:2020} and FILL IN REFERENCE TO LANNE AND VIROLAINEN WORKING PAPER show that if the following condition holds, the STVAR process is ergodic stationary (both strictly and second-order).
\begin{condition}\label{cond:sufficient}
$\rho(\lbrace \boldsymbol{A}_1,...,\boldsymbol{A}_M \rbrace) < 1$.
\end{condition}

Condition~\ref{cond:sufficient} is, however, computationally demanding the check in practice with a reasonable accuracy \citep[e.g.,][]{Chang+Blondel:2013}, making it impractical to use in the estimation. Therefore, we consider a necessary condition for Condition~\ref{cond:sufficient} that is easier to check in practice, which is that the usual stability condition is satisfied for each of the regimes. Specifically, we assume the following condition, which is analogous to Corollary~1 of \cite{Kheifets+Saikkonen:2020}, is necessary for Condition~\ref{cond:sufficient}.
\begin{condition}\label{cond:necessary}
$\max\lbrace \rho(\boldsymbol{A}_1),...,\rho(\boldsymbol{A}_M)\rbrace<1$,
\end{condition}
where $\rho(\boldsymbol{A}_m)$ is the spectral radius of $\boldsymbol{A}_m$, $m=1,...,M$.

Note that validity Condition~\ref{cond:necessary} does not imply the validity of Condition~\ref{cond:sufficient} that guarantees stationarity of the model. However, in practice models that satisfy Condition~\ref{cond:necessary} and are not very close to breaking this condition should most often satisfy Condition~\ref{cond:sufficient}. For checking the validity of Condition~\ref{cond:sufficient}, \citep{sstvars} implements the formula of \cite{Parrilo+Jadbabaie:2008} for the upper bound proposed by \cite{Blondel+Nesterov:2005} (the function \code{bound_JSR}. But unfortunately the calculations required are computationally very demanding in practice when the model is not small and even a relatively tight upper bound is required to verify Condition~\ref{cond:sufficient}. Therefore, in the cases where Condition~\ref{cond:sufficient} needs to be formally verified, we suggest using the \proglang{MATLAB} toolbox \pkg{JSR} by \cite{Jungers:2023}, as automatically combines several methods for bound the JSR and finds an accurate upper bound with a more reasonable computational effort.

\subsection{Specifications of the conditional distribution}\label{sec:cond_dist}

\subsubsection{Gaussian STVAR model}
Assuming the structural errors $e_t$ have standard normal distributions, the conditional distribution of $y_t$ conditional on $\mathcal{F}_{t-1}$ is Gaussian and characterized by the density function
\begin{equation}
f(y_t|\mathcal{F}_{t-1}) = n_d(y_t;\mu_{t},\Omega_{t})=(2\pi)^{-d/2}\det(\Omega_t)^{-1/2}\exp\left\lbrace -\frac{1}{2}(y_t - \mu_t)'\Omega_t^{-1}(y_t - \mu_t) \right\rbrace .
\end{equation}
That is, the conditional distribution is simply the $d$-dimensional Gaussian distribution with mean $\mu_t$ and covariance matrix $\Omega_t$. The Gaussian distribution simple and can be used with all of our transition weight functions, but in some cases it is useful to employ the more heavy tailed Student's $t$ distribution instead.

\subsubsection{Student's $t$ STVAR model}
To accommodate more heavy tailed data, instead of using Gaussian errors one may consider Student's $t$ errors and assume the shocks $e_t$ are Student's $t$ distributed with the mean zero, identity covariance matrix, and $\nu>2$ degrees of freedom (where the assumption $\nu>2$ is made to ensure the existence of second moments). The Student's $t$ STVAR model has the conditional distribution, conditional $\mathcal{F}_{t-1}$, characterized by the density function
\begin{equation}
f(y_t|\mathcal{F}_{t-1}) = t_d(y_t;\mu_t,\Omega_t,\nu)=C_d(\nu)\text{det}(\Omega_t)^{-1/2}\left(1+\frac{(y_t -\mu_t)'\Omega_t^{-1}(y_t - \mu_t)}{\nu-2}\right)^{-(d+\nu)/2},
\end{equation}
where
\begin{equation}
C_d(\nu)=\frac{\Gamma\left(\frac{d+\nu}{2}\right)}{\sqrt{\pi^d(\nu-2)^d}\Gamma\left(\frac{\nu}{2}\right)},
\end{equation}
and $\Gamma\left(\cdot\right)$ is the gamma function. The conditional distribution is, hence, the $d$-dimensional Student's $t$ distribution with mean $\mu_t$, covariance matrix $\Omega_t$, and $\nu$ degrees of freedom. Note that the parametrization differs from the conventional one, as the distribution is parametrized with a covariance matrix instead a scale matrix \cite[see, e.g.,][Appendix~A for details about the parametrization]{Meitz+Preve+Saikkonen:2023}.

The Student's $t$ errors are more flexible than the Gaussian ones, but they are not accommodated by the transition weights defined as weighted ratios of the regime's stationary densities (see Section~\ref{sec:rel_dens}). This is because they require the knowledge of the stationary distributions of the regimes corresponding to $p$ consecutive observations, and the stationary distribution is not known for the Student's $t$ regimes \citep[see, e.g.,]{Virolainen3:2021}. Moreover, Student's $t$ STVAR models are more difficult estimate in practice than the Gaussian ones, and estimation of the STVAR models can be demanding.

\subsection{Specifications of the transition weights}
Various specifications of the transition weights $\alpha_{m,t}$ can be considered to obtain smooth transition VARs with different properties. We assume that the transition weights are functions of $\lbrace y_{t-j}, j=1,...,p \rbrace$, because it is required for applicability of the stationarity condition discussed in Section~\ref{sec:stationarity}. Moreover, $\mathcal{F}_{t-1}$-measurability of the transition weights ensures that the true generalized impulse responses functions can be easily estimated, as completely exogenous switching-variables are excluded from affecting the weights.\footnote{Weaker forms of exogeneity of specific variables can, however, be imposed by constraining the AR matrices $A_{m,i}$, $m=1,...,M$, $i=1,...,p$, or the impact matrix $B_t$ accordingly (see Section~\ref{sec:examp_const})} We also assume that the transition weights are identical for all the individual equations in (\ref{eq:stvar1}) and (\ref{eq:stvar3}), which is required for applicability of the stationarity condition. Consequently, at each $t$‚ the process can be described as a weighted sum of linear VARs.

\subsubsection{Weighted relative likelihoods}\label{sec:rel_dens}
If the conditional distribution is specified to be Gaussian, weighted relative likelihoods of the regimes can be used as transition weights (FILL IN REFERENCE TO LANNE AND VIROLAINEN). In this specification, the transitions weights depend on the full distribution of the preceding $p$ observations, they are defined identically to the mixing weights in the Gaussian mixture vector autoregressive (GMVAR) model of \cite{Kalliovirta+Meitz+Saikkonen:2016}.
Denoting $\boldsymbol{y}_{t-1}=(y_{t-1},...,y_{t-p})$, the transition weights are defined as
\begin{equation}\label{eq:alpha_mt}
\alpha_{m,t} = \frac{\alpha_m n_{dp}(\boldsymbol{y}_{t-1};\boldsymbol{1}_p\otimes \mu_m, \boldsymbol{\Sigma}_{m,p})}{\sum_{n=1}^M \alpha_n n_{dp}(\boldsymbol{y}_{t-1};\boldsymbol{1}_p\otimes \mu_n, \boldsymbol{\Sigma}_{n,p})}, \ \ m=1,...,M,
\end{equation}
where $\alpha_1,...,\alpha_M$ are transition weight parameters that satisfy $\sum_{m=1}^M \alpha_m=1$ and $n_{dp}(\cdot;\boldsymbol{1}_p\otimes \mu_m, \boldsymbol{\Sigma}_{m,p})$ is the density function of the $dp$-dimensional Gaussian distribution with mean $\boldsymbol{1}_p\otimes \mu_m$ and covariance matrix $\boldsymbol{\Sigma}_{m,p}$. The symbol $\boldsymbol{1}_p$ denotes a $p$-dimensional vector of ones, $\otimes$ is Kronecker product, $\mu_m=(I_d - \sum_{i=1}^pA_{m,i})^{-1}\phi_{m,0}$, and the covariance matrix $\boldsymbol{\Sigma}_{m,p}$ is given in \citet[Equation~(2.1.39)]{Lutkepohl:2005}, but using the parameters of the $m$th regime. That is, $n_{dp}(\cdot;\boldsymbol{1}_p\otimes \mu_m, \boldsymbol{\Sigma}_{m,p})$ corresponds to the density function of the stationary distribution of the $m$th regime.

The transition weights are thus weighted ratios of the stationary densities of the regimes corresponding to the preceding $p$ observations. This specification is appealing, as it states that the greater the weighted relative likelihood of a regime is, the greater the weight of this regime is. The regimes are, hence, formed based on the statistical properties of the data and are not affected by the choice of the switching variables similarly to the logistic weights.

In the GMVAR model of \cite{Kalliovirta+Meitz+Saikkonen:2016}, the definition of the mixing weights also leads to attractive theoretical properties such as the the knowledge of the stationary distribution of $p+1$ consecutive observations. But this is not the case in our STVAR model, as the structure of the model is different. The STVAR model, in turn, has the advantage that is allows for smooth transitions between the regimes, and it is potentially easier to estimate in practice, as the conditional distribution is Gaussian as opposed to the mixture Gaussian distribution of the GMVAR model. The STVAR model also has the advantage in structural analysis that after the shocks have been identified, they can be recovered from the data. In the GMVAR model, the shocks cannot be generally recovered because the regime that generated each observation is not known. The GMVAR model has been implemented to the R package \pkg{gmvarkit} \cite{gmvarkit}, which works quite similarly to \pkg{sstvars}.

\subsubsection{Logistic transition weights}\label{sec:logistic_weights}
A common specification assumes logistic transition weights \citep[e.g.,][]{Anderson+Vahid:1998} that vary according to the level of the switching variable, which we assume to be a lagged endogenous variable. Here we assume that model has only two regimes ($M=2$), and in the next section, we show how the logistic weights generalize to multinomial logit weights that accommodate more regimes.

The logistic transition weights are defined as
\begin{align}
\alpha_{1,t} &= 1 - \alpha_{2,t},\\
\alpha_{2,t} &= [1 + \exp\lbrace -\gamma(y_{it-j} - c)\rbrace ]^{-1},
\end{align}
where $y_{it-j}$ is the $j$th lagged observation ($j\in \lbrace 1,...,p \rbrace$) of the $i$th variable ($i\in \lbrace 1,...,d \rbrace$), $c\in\mathbb{R}$ is a location parameter, and $\gamma > 0$ is a scale parameter. The location parameter $c$ determines the mid point of the transition function, i.e., the value of the (lagged) switching variable when the weights are equal. The scale parameter $\gamma$, in turn,  determines the smoothness of the transitions (smaller $\gamma$ implies smoother transitions), and it is assumed strictly positive so that $\alpha_{2,t}$ is increasing in $y_{it-j}$.

Compared to weighted relative likelihoods, an advantage of the logistic weights is that it allows to specify switching variables in a way that leads to the regimes the econometrician is interested in in a specific application. For instance, if one is interested in how the effects of the shocks vary along with business cycle fluctuations, $y_{it-j}$ may be set as a lagged output gap variable. STVAR models with logistics weights are also easier to estimate than those with the transition weights determined by weighted relative likelihoods of the regimes. A disadvantage is that the empirical results depend highly on the choice of the switching variable, and only the level of the switching variable affects the transition weights.

\subsubsection{Multinomial logit transition weights}
The logistic transition weights can be generalized to multinomial logit weights that accommodate more than two regimes as well as multiple lags of multiple switching variables as regressors in the logit sub model. The generality, however, comes at the cost of significantly more difficult estimation in the practice and loss of the intuitive interpretations of the parameters of the transition function.  With $M\geq 2$ regimes, we specify the multinomial logit weights as
\begin{equation}\label{eq:logistic_alphas}
\alpha_{m,t} = \frac{\exp\lbrace{\gamma_m'z_{t-1}\rbrace}}{\sum_{n=1}^M \exp\lbrace{\gamma_n'z_{t-1} \rbrace}}, \ \ m=1,...,M,
\end{equation}
where $z_{t-1}$ is an $(k\times 1)$ $\mathcal{F}_{t-1}$-measurable vector containing the (lagged) switching variables and a constant term, $\gamma_m$, $m=1,...,M-1$, are $(k\times 1)$ coefficient vectors, and the last one is normalized as $\gamma_M=0$ $(k\times 1)$ to facilitate identification.\footnote{\cite{Burgard+Neuenkirch+Nockel:2019} specify the mixing weights of their mixture VAR in a similar fashion, but unlike us, they allow for exogenous switching variables.}
Denote the set of switching variables as $I\subset \lbrace 1,...,d \rbrace$ (with the indices in $I$ corresponding to the ordering of the variables in $y_t$) and assume that $\tilde{p} \in \lbrace 1,...,p \rbrace$ lags are included in the transition weights. We assume
\begin{equation}
z_{t-1} = (1, \tilde{z}_{\min\lbrace I\rbrace},...,\tilde{z}_{\max\lbrace I\rbrace}), \ \ \ \tilde{z}_{j} =(y_{it-1},...,y_{it-\tilde{p}}), \ \ i\in I.
\end{equation}
So $k=1+|I|\tilde{p}$ where $|I|$ is the cardinality of the set $I$ (i.e., the number of elements in $I$). For instance, if the switching variables are the first two variables in $y_t$, $I=\lbrace 1,2 \rbrace$ and only the first lag is included, $\tilde{p}=1$, we have $z_{t-1} = (1,y_{1t-1}, y_{2t-1})$.

The specification implies
\begin{equation}
\log\frac{\alpha_{m,t}}{\alpha_{M,t}}=\gamma_m'z_{t-1}, \ \ m=1,...,M-1.
\end{equation}
Our specification assumes that all the lags up to the lag $\tilde{p}$ are included in the transition weights. The inclusion of only some specific lags in the transition weights is, however, accommodated by imposing constraints on the parameters $\alpha \equiv (\gamma_1,...,\gamma_{M-1})$ $((M-1)k\times 1)$. Specifically, \pkg{sstvars} assumes constraints of the form
\begin{equation}\label{eq:weightconstraints}
\alpha = R\xi + r,
\end{equation}
where $R$ is a known $((M-1)k\times l)$ constraint matrix, $r$ is a known $((M-1)k\times 1)$ constant, and $\xi$ is an unknown $(l \times 1)$ parameter. For instance, by assuming that $R$ is a matrix of zeros, the weight parameter $\alpha$ can be constrained to a known constant.

The logistic weights discussed in the previous section, with $y_{it-j}$ as the switching variable for some lag $j\in \lbrace 1,...,p \rbrace$ and $i\in \lbrace 1,...,d \rbrace$, are obtained as a special case as follows. Assume $M=2$, $\tilde{p}=j$, and $I=\lbrace i\rbrace$, so that $z_{t-1}=(1, y_{it-1},...,y_{it-j})$. Then, impose the constraints $r=0$ and
\begin{equation}
R=
\begin{bmatrix}
1 & 0 \\
0 & 0 \\
\vdots & \vdots \\
0 & 1 \\
\end{bmatrix}
\ \ (j+1 \times 2),
\end{equation}
so $\xi = (\gamma_{1,1},\gamma_{j+1,1})$, where $\gamma_{l,m}$ is the $l$th element of $\gamma_{m}$.

A direct calculation shows that the "scale parameter" is $-\gamma_{j+1,1)}$ and the "location parameter" is $\frac{\gamma_{1,1}}{-\gamma_{j+1,1}}$. The linear constraints (\ref{eq:weightconstraints}) do not, however, enable to constrain the location parameter $\frac{\gamma_{1,1}}{-\gamma_{j+1,1)}}$ to a specific value while leaving the scale parameter $-\gamma_{(j+1),1)}$ unconstrained (or vice versa), or to constrain the scale parameter strictly positive. Therefore, it is more convenient to use the logistic weights and parametrization discussed in Section~\ref{sec:logistic_weights} when only two regimes and one lag of one switching variable are used.

\subsubsection{Exponential transition weights}
Exponential transition weights \citep[see, e.g.,][]{Terasvirta:1994} vary according to the level of the switching variable, which we assume to be a lagged endogenous variable. Similarly to the logistic transition weights discussed in Section~\ref{sec:logistic_weights}, the exponential weights depend on a location parameter $c$ and a scale parameter $\gamma$ that determine the mid point of the transition curve and smoothness of the transitions, respectively. But instead of logistic transition function, we consider an exponential transition function.

Specifically, we assume $M=2$ and define the exponential transition weights as
\begin{align}
\alpha_{1,t} &= 1 - \alpha_{2,t},\\
\alpha_{2,t} &= 1 - \exp\lbrace -\gamma(y_{it-j} - c)^2\rbrace
\end{align}
where $y_{it-j}$ is the $j$th lagged observation ($j\in \lbrace 1,...,p \rbrace$) of the $i$th variable ($i\in \lbrace 1,...,d \rbrace$), $c\in\mathbb{R}$ is a location parameter, and $\gamma > 0$ is a scale parameter. The location parameter $c$ determines the value of the (lagged) switching variable when the process is completely in first regime, i.e., $\alpha_{1,t}=1$ and $\alpha_{2,t}=0$. The closer $y_{it-j}$ is to $c$, the greater the weight of the first regime is. Conversely, when the deviation of $y_{it-j}$ from $c$ increases, the weight of the second regime increases (and the weight of the first regime decreases). The scale parameter $\gamma$, in turn, determines the smoothness of the transitions (smaller $\gamma$ implies smoother transitions), and it is assumed strictly positive so that $\alpha_{2,t}\in[0,1]$ for all $y_{it-j}$.

\subsubsection{Threshold transition weights}
Threshold transition weights assume discrete regime switches such that the regime switches when the level of the switching variable exceeds or falls below a threshold value. This type model nonlinear VARs are often referred to as Threshold VAR (TVAR) models \citep{Tsay:1998} or so-called slef-exciting TVAR models (due to the endogenous switching-variable). We interpret the TVAR model as a special case of the STVAR models, despite of the regime switches being discrete rather than smooth.

For a model with $M>1$ regimes, consider the $M-1$ threshold values $r_1,.,..,r_{M - 1}\in\mathbb{R}$ such that $r_1<\cdots<r_{M-1}$, and suppose the switching variable is $i\in \lbrace 1,...,d \rbrace$ with the lag $j\in \lbrace 1,...,p \rbrace$. The transition function is defined as
\begin{equation}\label{eq:alpha_mt_threshold}
\alpha_{m,t} =
\left\lbrace\begin{matrix}
1 & \text{if} \ \ r_{m-1} < y_{it-j} \leq r_{m}, \\
0 & \text{otherwise}, \phantom{aaaaaaaaa}
\end{matrix}\right.
\end{equation}
where $r_0\equiv-\infty$, $r_M\equiv\infty$, and $m=1,...,M$. In other words, at each $t$‚ the model defined in Equations~(\ref{eq:stvar1})-(\ref{eq:stvar3}) and (\ref{eq:alpha_mt_threshold}) reduces to a linear VAR corresponding to one the regimes that is determined according to the level of the switching variable $y_{it-j}$.

Compared to smoothly varying transition weights, threshold transition weights have the advantage that the resulting model is easier to estimate in practice and the regimes have clear interpretations. An obvious disadvantage is the inability to capture smooth transition between the regimes or more complex regime-switching dynamics that depend on other factors than just on the level of the switching variable.

\section{Structural STVAR models}\label{sec:struct_stvar}
The STVAR models specified in (\ref{eq:stvar1})-(\ref{eq:stvar3}) directly incorporates the orthogonal, identically and independently distributed structural shocks $e_t$. The shocks are identified by finding an impact matrix $B_t$ that satisfies Equation~(\ref{eq:stvar3}) for all $t$ and recovers the shocks of interest. There are in general multiple solutions to $B_t$, since there are $d^2$ variables but only $d(d+1)/2$ unique equations in (\ref{eq:stvar3}). Therefore, further restrictions are required for identification of the shocks. Currently, \pkg{sstvars} supports two types of identification methods: recursive identification by the lower-triangular Cholesky decomposition and identification by conditional heteroskedasticity. Other identification methods will be likely added the package in the future.

\subsection{Recursive identification}
A conventional way of identifying the shocks is to impose restrictions on the impact responses of variables. A commonly applied identification is to assume a recursive lower-triangular structure on $B_t$, implying that $B_t$ is obtained as the Cholesky decomposition of the conditional covariance matrix $\Omega_{y,t}$. The recursive identification is straightforward to apply and it allows many of the impact responses to vary in time but constraints many of them to zero. This is particularly disadvantageous if the shock of interest is ordered last or almost last (which is typically the case in small-scale monetary policy shock applications), as then the impact responses of many of the variables to the shock of interest are zero, and therefore, time-invariant.

With our specification of the STVAR model, recursive identification does not, however, allow to impose over-identifying restrictions on the impact matrix $B_t$. This is because there does not exist a direct parametrization of a lower-triangular $B_t$ such that the conditional covariance matrix $\Omega_{y,t}$ is a weighted sum of the regime-specific covariance matrices with time-varying weights.

\subsection{Identification via heteroskedasticity}
An alternative identification method proposed by \cite{Lutkepohl+Netsunajev:2017} for structural VARs with smooth transitions in variances \citep[see also the seminal paper by][]{Rigobon:2003} identifies the shocks by simultaneously diagonalizing the covariance matrices $\Omega_1,...,\Omega_M$. This restricts the relative impact responses of the variables to be constant over time (for each shock) but does not necessarily require any zero restrictions. Since \cite{Lutkepohl+Netsunajev:2017} assume only two regimes and do not normalize the conditional covariance matrix of the structural error to a constant, their specification does not directly apply to our model. Therefore, we adopt the more suitable specification of \cite{Virolainen2:2021}, and decompose the covariance matrices as
\begin{equation}\label{eq:decomp}
\Omega_m=W\Lambda_mW', \quad m=1,...,M,
\end{equation}
where the diagonal of $\Lambda_m=\text{diag}(\lambda_{m1},...,\lambda_{md})$, $\lambda_{mi}>0$ ($i=1,...,d$), contains the eigenvalues of the matrix $\Omega_m\Omega_1^{-1}$ and the columns of the nonsingular $W$ are the related eigenvectors (that are the same for all $m$ by construction). When $M=2$, the decomposition (\ref{eq:decomp}) always exists \citep[Theorem A9.9]{Muirhead:1982}, but for $M>2$ its existence requires that the matrices $\Omega_m\Omega_1^{-1}$ share the common eigenvectors in $W$. This is, however, testable.

The impact matrix is then obtained as
\begin{equation}
B_t=W\left(\sum_{m=1}^M\alpha_{m,t}\Lambda_m\right)^{1/2},
\end{equation}
where $\Lambda_1=I_d$. The shocks are identified up to ordering and sign if none of the pairs of $\lambda_{mi}$, $i=1,...,d$, is identical for all $m=2,...,M$. Assuming that his condition is satisfied, the shocks can be labelled according to the unrestricted impact responses on $B_t$, and if necessary, further economically motivated restrictions can be imposed. The additional economic restrictions on the impact matrix testable, as they are overidentifying. See \cite{Virolainen2:2021} for a more detailed discussion on the identification and labeling of the shocks.

Shocks identified by heteroskedasticity impose constant relative impact responses for the variables, making them unsuitable for some applications concerned with time-varying impulse response functions. Similarly to the recursive identification, this method is straightforward to apply, but unlike the recursive identification, it does not necessarily require any zero constraints on the impact responses. Compared to the recursive identification, identification by heteroskedasticity is therefore particularly advantageous when the recursive identification would imply that the shock of interest is order last. In this case, the assumption of time-invariant relative impact responses is less restrictive than the zero restrictions of the recursive identification. In contrast, the recursive identification is particularly appealing when the shock of interest is ordered first (or almost first), as then the impact responses to the shock of interest can vary freely in time.

\cite{Bacchiocchi+Fanelli:2015}, \cite{Bacchiocchi+Castelnuovo+Fanelli:2016}, and \cite{Angelini+Bacchiocchi+Caggiano+Fanelli:2019} propose alternative way of identifying the shocks by combining heteroskedasticity with zero restrictions on the impact matrices. Their method allows some of the impact responses vary in time also relative to the other variables, but the required untestable zero constraints may challenging to justify economically. But since their identification method is not directly applicable to the STVAR models specified in Section~\ref{sec:models}, we do not discuss it further.

\section{Estimation}\label{sec:estimation}
The parameters of the reduced form STVAR model are collected to the vector
\begin{equation}\label{eq:paramvector}
\boldsymbol{\theta}=(\phi_{1,0},...,\phi_{m,0},\varphi_1,...,\varphi_M,\sigma,\alpha,\nu),
\end{equation}
where $\varphi_m=(\text{vec}(A_{m,1}),....,\text{vec}(A_{m,p}))$, $m=1,...,M$, $\sigma=(\text{vech}(\Omega_1),...,\text{vech}(\Omega_M))$, $\alpha$ contains the transition weight parameters, $\nu$ is the degrees of freedom parameter (which is dropped for Gaussian models), vec is a vectorization operator that stacks the columns of a matrix on top of each other, and vech stacks the columns of a matrix from the main diagonal downwards (including the main diagonal). Structural models identified by heteroskedasticity assume $\sigma = (\text{vec}(W),\lambda_m,...,\lambda_M)$, $\lambda_m=(\lambda_{m1},...,\lambda_{md})$, $m=2,...,M$, whereas recursively structural models use the same parameter vector as reduced form models.

If relative stationary densities are used as transition weights, $\alpha=(\alpha_1,...,\alpha_{M-1})$ ($\alpha_M$ is not included because it is obtained from the constraint $\sum_{m=1}^M \alpha_m=1$), where we assume, for identification, that $\alpha_1,...,\alpha_{M-1}$ in a decreasing order. For exponential and logistic transition weights, $\alpha=(c,\gamma)$, for multinomial logit transition weights  $\alpha=(\gamma_1,...,\gamma_{M-1})$ ($\gamma_M$ is not included because $\gamma_M=0$ is assumed for identification), and with trhesold transition weights, $\alpha=(r_1,...,r_{M-1})$.

\subsection{Log-likelihood function}\label{sec:loglik}
\pkg{sstvars} employs the method of maximum likelihood (ML) for estimating the parameters of the STVAR models. Indexing the observed data as $y_{-p+1},...,y_0,y_1,...,y_T$, the conditional log-likelihood function conditional on the initial values $\boldsymbol{y}_0=(y_{-p+1},...,y_0)$ is given as
\begin{equation}\label{eq:loglik1}
L_t(\boldsymbol{\theta})=\sum_{t=1}^T l_t(\boldsymbol{\theta}) = \sum_{t=1}^T \log d_d(y_t;\mu_{y,t},\Omega_{y,t},\nu).
\end{equation}
where $d_d(y_t;\mu_{y,t},\Omega_{y,t},\nu)$ is the $d$-dimensional conditional density of the process, conditional on $\mathcal{F}_{t-1}$, at time $t$, given in Section~\ref{sec:cond_dist}. When the conditional distribution is Gaussian, the degrees of freedom parameter $\nu$ is dropped from the right side of (\ref{eq:loglik1}). That is, the ML estimator of $\boldsymbol{\theta}$ maximizes the log-likelihood function $L_t(\boldsymbol{\theta})$ over the parameter space.

We summarize the constraints imposed on the parameter space in the following assumption.
%
\begin{assumption}\label{as:mle}
The true parameter value $\boldsymbol{\theta}_0$ is an interior point of $\boldsymbol{\Theta}$, which is a compact subset of
$\lbrace \boldsymbol{\theta}=(\phi_{1,0},...,\phi_{m,0},\varphi_1,...,\varphi_M,\sigma,\alpha,\nu)\in\mathbb{R}^{M(d + d^2p + d(d+1)/2)}\times S\times (2,\infty):$ $\Omega_m$ is positive definite for all $m=1,...,M$, and Condition~\ref{cond:sufficient} holds.
$\rbrace$.
\end{assumption}
%
Above, $S=(0,1)^{M-1}$ for relative densities transition weights, $S=\mathbb{R}\times (0,\infty)$ for logistic and exponential weights, $S=\mathbb{R}^{(M-1)k}$ for multinomial logistic weights, and $S=\mathbb{R}^{M-1}$ for threshold weights. As noted before, Condition~\ref{cond:sufficient} is not necessary, but it ensures stationarity and ergodicity of the process. In estimation, we impose the more easily verified Condition~\ref{cond:necessary}, however, and the sufficient condition can be check after the estimation, if necessary. As noted in Section~\ref{sec:stationarity}, the sufficient condition is in practice likely satisfied if the necessary condition is satisfied and not very close to being violated. Given that under Condition~\ref{cond:stationarity} the process is ergodic stationary, there is not particular reason to believe that the standard asymptotic results of consistency and limiting Gaussian distribution would not apply to the ML estimator.

\subsection{Two-phase estimation procedure}\label{sec:estimscheme}
Finding the ML estimate amounts maximizing the log-likelihood function (\ref{eq:loglik1}) over a high dimensional parameter space satisfying the constraints summarized in Assumption~\ref{as:mle}. Due to the complexity of the log-likelihood function, numerical optimization methods are required. The maximization problem can be challenging in practice due to the dependence of the transition weights on the preceding observations, which induces a large number of modes to the surface of the log-likelihood function, and large areas to the parameter space, where it is flat in multiple directions.

Therefore, we follow \cite{Meitz+Preve+Saikkonen:2023} and \cite{Virolainen:2022} and employ a two-phase estimation procedure that is run for a large number of times. In the first phase, a genetic algorithm is used to find parameter values (hopefully) near local maximums. Since genetic algorithms tend to converge slowly near local solutions, a gradient based variable algorithm \cite[algorithm 21, implemented by \citealp{R}]{Nash:1990} is ran for each of the starting values, resulting in a number of alternative local solutions. Some of the estimation rounds may end up in saddle points or near-the-boundary points that are not local solutions, and some of the local solutions may be  inappropriate for econometric inference (for instance, there might be only a few observations from some of the regimes). After the estimation rounds have been ran, the researcher can choose the local solution that maximizes the log-likelihood among the appropriate local solutions. Inappropriate solutions are automatically filtered by \pkg{sstvars}, but this functionality can also be turned off and the researchers can use estimates based any estimation round. The R package \pkg{sstvars} employs a modified genetic algorithm that works similarly to the one described in the R packages \pkg{uGMAR} \citep{uGMAR} and \pkg{gmvarkit} \citep{gmvarkit} \citep[the genetic algorithm and iemployed in former is briefly described in][]{Virolainen:2022}. See \citet[Chapter~3]{Virolainen2:2022} for a related discussion on complex numerical estimation problems using the two-phase procedure.

\subsection{Examples of unconstrained estimation}\label{sec:example_estim}
In this section, we demonstrate how to estimate STVAR models with \pkg{sstvars}. In the examples, we only consider $p=1$ models for simplicity and merely because then the code outputs fit in the margins better. This order may not be best in the modeling perspective, however.

In \pkg{sstvars}, the STVAR models are defined as class \code{stvar} S3 objects, which can be created with given parameter values using the constructor function \code{STVAR} (see Section~\ref{sec:STVAR}) or by using the estimation function \code{fitSTVAR}, which estimates the parameters and then builds the (reduced form) model. Structural models are estimated based on a reduced form model with the function \code{fitSSTVAR}. For estimation, \code{fitSTVAR} needs to be supplied with a multivariate time series and the arguments specifying the model. The necessary arguments for specifying the model include the autoregressive order \code{p}, the number of regimes \code{M}, the transition weight function \code{weight_function}, with some weight functions the switching variable(s) \code{weightfun_pars}, and the conditional distribution \code{cond_dist}.

Additional arguments may be supplied to \code{fitSTVAR} in order to specify, most importantly, how many estimation rounds should be performed (\code{nrounds}) and how many central processing unit (CPU) cores should be used in the estimation (\code{ncores}). Some of the estimation rounds may end up in local-only maximum points or saddle points, but reliability of the estimation results can be improved by increasing the number of estimation rounds. A large number of estimation rounds may be required particularly when the number of regimes is large or there are many variables in the data, as the surface of the log-likelihood function becomes increasingly more challenging. It is also possible to adjust the settings of the genetic algorithm that is used to find the starting values. The available options are listed in the documentation of the function \code{GAfit} to which the arguments adjusting the settings will be passed.

\textbf{In general, we recommend being conservative with choice of M due to the identifation problems induced if the number of regimes is chosen too large. Also, estimation of models that contain more than two regimes can be extremely challenging. Another important thing to know about estimation is that the estimation algorithm performs very poorly if some of the AR coefficients very large, signifcantly larger than one. This means that you need scale each component time series so that they vary approximately in the same magnitude. For instance, typically in macroeconomic time series, log-differences should be multiplied by hundred. If the suitable scales are not obvious, you can try out different scales and estimate linear VARs with your favorite package to see whether the AR coeffients are in a reasonable range. When a suitable scale is found, proceed to the STVAR models.}

We illustrate the use of \pkg{sstvars} with a quarterly series consisting of two U.S. variables: the percentage change of real GDP and the percentage change of GDP implicit price deflator, covering the period from 1959Q1 to 2019Q4. The following code fits a STVAR($p=1,M=2$) model with Student's $t$ conditional distribution and logistic transition weights by performing $24$ estimation rounds with $8$ CPU cores.\textbf{In practice, hundreds or even thousands of estimation rounds is often required to obtain reliable results. The larger the dimension of the series is and the larger the order of the model is and the more there are regimes, the more estimation rounds is required. The model in our example is easy to estimate, as it is small in dimension and order.} We set the switching variable to be the first lag of the second variable, i.e., the GDP deflator by setting argument \code{weightfun_pars=c(2, 1)}.

The argument \code{seeds} supplies the seeds that initialize the random number generator at the beginning of each call to the genetic algorithm, thereby yielding reproducible results.
%
\begin{CodeChunk}
\begin{CodeInput}
R> library(sstvars)
R> data("gdpdef", package="sstvars")
R> fit12t <- fitSTVAR(gdpdef, p=1, M=2, weight_function="logistic",
+    weightfun_pars=c(2, 1), cond_dist="Student", nrounds=24, ncores=8,
+    seeds=1:24)
\end{CodeInput}
\begin{CodeOutput}
Using 8 cores for 24 estimations rounds...
Optimizing with a genetic algorithm...
  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% elapsed=31s
Results from the genetic algorithm:
The lowest loglik:  -283.893
The largest loglik: -253.864
Optimizing with a variable metric algorithm...
  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% elapsed=03s
Results from the variable metric algorithm:
The lowest loglik:  -276.669
The largest loglik: -250.236
Filtering inappropriate estimates...
Calculating approximate standard errors...
Finished!
\end{CodeOutput}
\end{CodeChunk}
%
The progression of the estimation process is reported with a progress bar giving an estimate of the remaining estimation time. Also statistics on the spread of the log-likelihoods are printed after each estimation phase. The progress bars are generated during parallel computing with the package \pkg{pbapply} \citep{Solymos+Zawadzki:2020}.

Because the log-likelihood function is highly multimodal, and the estimation algorithm is ran a large number of times, it produces a set of local solutions, possibly representing various modes in the log-likelihood function. Some of the local solutions may be inappropriate for econometric inference, for instance, because they contain a near-singular error term covariance matrix or regimes that have only a very small number of observations generated (even partially) from them. Such inappropriate solutions, i.e., estimates that are not solutions of interest, are filtered automatically by \pkg{sstvars}. Specifically, solutions that incorporate a near-singular error term covariance matrix (any eigenvalue less than $0.002$) or transition weights such that they are close to zero for almost all $t$ for at least one regime. Solution very close to the boundary of the stationarity region are not filtered automatically, because sometimes the series are very persistent and the solutions are very close to the boundary. In the case of a close-to-the-boundary solution, the researcher can check for alternative local solutions using the function \code{alt_stvar}.

Automatic filtering can be turned this off with the argument \code{filter_estimates=FALSE}. Then, the various local solutions can be easily examined by hand by using the function \code{alt_stvar} and adjusting its argument \code{which_largest} or \code{which_round}. Note that \code{alt_stvar} can be used also when the automatic filtering is turned on, since the estimates from all of the estimation rounds are stored in the class \code{'stvar'} object returned by \code{fitSTVAR}.

The estimates can be examined with the \code{print}.
%
\begin{CodeChunk}
\begin{CodeInput}
R> print(fit12)
\end{CodeInput}
\begin{CodeOutput}
logistic Student STVAR model, reduced form model no AR_constraints, no mean_constraints,
  p = 1, M = 2, d = 2, #parameters = 21, #observations = 243 x 2
  Switching variable: GDPDEF with lag 1.

Regime 1
Degrees of freedom: 7.70 (for all regimes)
Regime means: 0.71, 0.49

   Y     phi0          A1                  Omega        1/2
1 y1 = [ 0.63 ] + [  0.35 -0.35 ] y1.1 + [  0.37 0.00 ]     eps1
2 y2   [ 0.14 ]   [  0.06  0.62 ] y2.1   [  0.00 0.03 ]     eps2

Regime 2
Weight params: 1.22 (location), 5.01 (scale)
Regime means: 0.77, 1.76

   Y     phi0          A1                  Omega         1/2
1 y1 = [ 2.41 ] + [  0.13 -0.99 ] y1.1 + [  1.29 -0.06 ]     eps1
2 y2   [ 0.67 ]   [ -0.04  0.64 ] y2.1   [ -0.06  0.19 ]     eps2
\end{CodeOutput}
\end{CodeChunk}
%
The parameter estimates are reported for each mixture component separately so that the estimates can be easily interpreted. Each regime's autoregressive formula is presented in the form
\begin{equation}
y_t = \varphi_{m,0} + A_{m,1}y_{t - 1} + ... + A_{m,p}y_{t - p} + \Omega_{m}^{1/2}\varepsilon_{t}.
\end{equation}
If $\Omega_{m,t}^{1/2}$ is time varying, it printed in the form $\Omega_{m,t}^{1/2}=(\text{ARCH\_mt}\Omega_m)^{1/2}$ where $\text{ARCH\_mt}$ is the $\omega_{m,t}$ defined in (\ref{eq:sigma_mt}). No numerical value is given to the ARCH scalar, as it is time-varying. The other statistics are listed above the formula, including the mixing weight pameter $\alpha_m$, the unconditional mean $\mu_m$, and the degrees freedom parameter $\nu_m$.

The above printout shows that the second regime's degrees of freedom parameter estimate is very large, which might induce numerical problems. However, since a StMVAR model with some degrees of freedom parameters tending to infinity coincides with the G-StMVAR model with the corresponding regimes switched to GMVAR type, one may avoid the problems by switching to the appropriate G-StMVAR model \citep[see][]{Virolainen2:2021}. Switching to the appropriate G-StMVAR model is recommended also because it removes the redundant degrees of freedom parameters from the model, thereby reducing its complexity. The function \code{stmvar_to_gstmvar} does this switch automatically by first removing the large degrees of freedom parameters and then estimating the G-StMVAR model with a variable metric algorithm \citep[algorithm 21]{Nash:1990} using the induced parameter vector as the initial value.

To exemplify, the following code switches all the regimes of the StMVAR model \code{fit12t_alt} with a degrees of freedom parameter estimate larger than $100$ to GMVAR type, and then estimates the corresponding G-StMVAR model.
%
\begin{CodeChunk}
\begin{CodeInput}
R> fit12gs <- stmvar_to_gstmvar(fit12t_alt, maxdf=100)
\end{CodeInput}
\end{CodeChunk}
%
We use the \code{summary} method to obtain a more detailed printout of the estimated the G-StMVAR model:
%
\begin{CodeChunk}
\begin{CodeInput}
R> summary(fit12gs)
\end{CodeInput}
\begin{CodeOutput}
Reduced form G-StMVAR model:
 p = 1, M1 = 1, M2 = 1, d = 2, #parameters = 20, #observations = 244 x 2,
 conditional log-likelihood, intercept parametrization, no AR parameter
 constraints

 log-likelihood: -247.50, AIC: 534.99, HQIC: 563.13, BIC: 604.85

Regime 1 (GMVAR type)
Moduli of 'bold A' eigenvalues:  0.75, 0.10
Cov. matrix 'Omega' eigenvalues: 1.21, 0.14
Mixing weight: 0.17
Regime means: 0.66, 1.67

   Y     phi0          A1                  Omega         1/2
1 y1 = [ 1.60 ] + [  0.13 -0.61 ] y1.1 + [  1.21 -0.04 ]     eps1
2 y2   [ 0.48 ]   [ -0.03  0.72 ] y2.1   [ -0.04  0.14 ]     eps2

Error term correlation matrix:
       [,1]   [,2]
[1,]  1.000 -0.087
[2,] -0.087  1.000

Regime 2 (StMVAR type)
Moduli of 'bold A' eigenvalues:  0.70, 0.34
Cov. matrix 'Omega' eigenvalues: 0.42, 0.04
Mixing weight: 0.83
Regime means: 0.78, 0.54
Df parameter:  7.57

   Y     phi0          A1                            Omega          1/2
1 y1 = [ 0.55 ] + [  0.33 -0.04 ] y1.1 + (         [  0.42 0.00 ] )     eps1
2 y2   [ 0.12 ]   [  0.05  0.71 ] y2.1   ( ARCH_mt [  0.00 0.04 ] )     eps2

Error term correlation matrix:
      [,1]  [,2]
[1,] 1.000 0.014
[2,] 0.014 1.000

Print approximate standard errors with the function 'print_std_errors'.
\end{CodeOutput}
\end{CodeChunk}
%
In the G-StMVAR model, estimates for GMVAR type regimes are reported before StMVAR type regimes, in a decreasing order according to the mixing weight parameter estimates. As shown above, the model \code{fit12gs} incorporates one GMVAR type regime and one StMVAR type regime. Estimates of the unconditional mean, the first $p$ autocovariances and autocorrelations (including the unconditional covariance matrix) can be obtained from the element \code{\$uncond_moments} of the model object. The conditional moments calculated using the data are available for the process (\code{\$total_cmeans} and \code{\$total_ccovs}) as well as for the regimes separately (\code{\$regime_cmeans} and \code{\$regime_ccovs}). These conditional moments can be conveniently plotted along the series with the function \code{cond_moment_plot}.

Approximate standard errors can be printed with the function \code{print_std_errors}, which prints the standard errors in the same form as the print method prints the estimates. Note that the last mixing weight parameter estimate does not have an approximate standard error because it is not parametrized. Likewise, there is no standard error for the intercepts if mean parametrization is used (by setting \code{parametrization = "mean"} in \code{fitGSMVAR}) and vice versa. In order to obtain standard errors for the regimewise unconditional means or intercepts, one can easily swap between the mean and intercept parametrizations with the function \code{swap_parametrization}.

To exemplify, the following code prints approximate standard errors for the model \code{fit12gs}:
%
\begin{CodeChunk}
\begin{CodeInput}
R> print_std_errors(fit12gs)
\end{CodeInput}
\begin{CodeOutput}
Reduced form model:
p = 1, M = 2, conditional log-likelihood, intercept parametrization,
no AR parameter constraints

APPROXIMATE STANDARD ERRORS

Regime 1 (GMVAR type)
Mixing weight: 0.124

   Y      phi0          A1                  Omega         1/2
1 Y1 = [ 0.759 ] + [ 0.149 0.386 ] Y1.1 + [ 0.264 0.061 ]     eps1
2 Y2   [ 0.234 ]   [ 0.050 0.118 ] Y2.1   [ 0.061 0.029 ]     eps2

Regime 2 (StMVAR type)
Df parameter:  2.740

   Y      phi0          A1                            Omega         1/2
1 Y1 = [ 0.127 ] + [ 0.079 0.196 ] Y1.1 + (         [ 0.070 0.011 ]     eps1
2 Y2   [ 0.037 ]   [ 0.024 0.064 ] Y2.1   ( ARCH_mt [ 0.011 0.008 ]     eps2
\end{CodeOutput}
\end{CodeChunk}
%

Missing values are reported when \pkg{gmvarkit} is not able to calculate the standard error. This typically happens either because there is an overly large degrees of freedom parameter estimate in the model or because the estimation algorithm did not stop a local maximum. In the former case, switch to the appropariate G-StMVAR with the function \code{stmvar_to_gstmvar}. In the latter case, make sure the estimate in not an unreasonable near-the-boundary point. If it is, it might appropriate the consider the next-best local maximum with the function \code{alt_gsmvar}. If it is not a near-the-boundary point, try running more iterations of the variable metric algorithm with the function \code{iterate_more}. Section~\ref{sec:examestim} discusses how to evaluate with \pkg{gmvarkit} whether the estimate is a local maximum (and how to improve the reliability that it is the global maximum).

Other statistics reported in the summary printout include the log-likelihood and values of the information criteria, moduli of the eigenvalues of the 'bold A' matrix (see (\ref{eq:gausmatrices})) and eigenvalues of the covariance matrix $\Omega_m$. If some of the moduli are very close to one, the related estimates are near the boundary of the stationarity region. If some of the eigenvalues of $\Omega_m$ close to zero, the related estimates are near the boundary of positive-definiteness region. As mentioned already multiple times, this kind of near-the-boundary point might be unreasonable and maximize the log-likelihood function for a technical reason, so it might be more appropriate to consider the next-best local maximum with the function \code{alt_gsmvar}.

This is possible in \pkg{gmvarkit}, because the estimation function \code{fitGSMVAR} stores the estimates from all the estimation rounds so that a GSMVAR model can be built based on any one of them, most conveniently with the function \code{alt_gsmvar}. The desired estimation round can be specified either with the argument \code{which_round} or \code{which_largest}. The former specifies the round in the estimation order, whereas the latter specifies it in a decreasing order of the log-likelihoods.

It is also possible to automatically filter out inappropriate estimates by setting the argument \code{filter_estimates=TRUE} in \code{fitSGMVAR}. Then, the function will automatically filter out estimates that it deems "inappropriate". That is, estimates that are not likely solutions of interest. Specifically, solutions that incorporate a near-singular error term covariance matrix (any eigenvalue less than $0.002$), mixing weights such that they are close to zero for almost all $t$ for at least one regime, or mixing weight parameter estimate close to zero (or one). It also filters out estimates with any modulus "bold A" eigenvalues larger than 0.9985, as the solution is near the boundary of the stationarity region and likely not a local maximum. \code{fitSGMVAR} then returns the solution based on the largest log-likelihood that is not filtered out. Other solutions can be studied by using the function \code{alt_gsmvar} as usual.

\subsection{Further examination of the estimates}\label{sec:examestim}
In addition to examining the summary printout, it is often useful to visualize the model by plotting the mixing weights together with the time series and the model's (marginal) stationary density together with a kernel density estimate of the time series. That is exactly what the plot method for GSMVAR models does. The following command creates the time series plot along with estimated mixing weights:
%
\begin{CodeChunk}
\begin{CodeInput}
R> plot(fit12gs, type="series")
\end{CodeInput}
\end{CodeChunk}
%
The resulting plot is presented in Figure~\ref{fig:seriesplot}.

%\begin{figure}[p]
%  \centering
%  \includegraphics{figures/seriesplot.png}
%  \caption{The figure produced by the command \code{plot(fit12gs, type="series")}. On the top, a quarterly series consisting of two U.S. variables: the percentage change of real GDP and the percentage change of GDP implicit price deflator, covering the period from 1959Q1 to 2019Q4. On the bottom, the estimated mixing weights of the G-StMVAR model \code{plot(fit12gs)} fitted the series.}
%\label{fig:seriesplot}
%\end{figure}

And the following command creates the stationary density plot:
%
\begin{CodeChunk}
\begin{CodeInput}
R> plot(fit12gs, type="density")
\end{CodeInput}
\end{CodeChunk}
%
The resulting plot is presented in Figure~\ref{fig:densityplot}. If the argument \code{type} is not specified, both of the figures will be plotted.

%\begin{figure}[p]
%  \centering
%  \includegraphics{figures/densityplot.png}
%  \caption{The figure produced by the command \code{plot(fit12gs, type="density")}. Kernel density estimates of the marginal series of the data the model was fitted to (black solid line), the stationary marginal density of the estimated G-StMVAR model (grey dashed line), and the marginal stationary densities of the component processes multiplied by the mixing weight parameter estimates (blue and red dashed lines).}
%\label{fig:densityplot}
%\end{figure}

It is also sometimes interesting to examine the time series of (one-step) conditional means and variances of the process along with the time series the model was fitted to. This can be done conveniently with the function \code{cond_moment_plot}, where the argument \code{which_moment} should be specified with \code{"mean"} or \code{"variance"} accordingly. In addition to the conditional moment of the process, \code{cond_moment_plot} also displays the conditional means or variances of the regimes multiplied by the mixing weights. Note, however, that the conditional variance of the process is not generally the same as the weighted sum of regimewise conditional variances, as it includes a component that encapsulates heteroskedasticity caused by variation in the conditional mean.

The variable metric algorithm employed in the final estimation does not necessarily stop at a local maximum point. The algorithm might also stop at a saddle point or near a local maximum, when the algorithm is not able to increase the log-likelihood, or at any point, when the maximum number of iterations has been reached. In the latter case, the estimation function throws a warning, but saddle points and inaccurate estimates need to be detected by the researcher.

It is well known that in a local maximum point, the gradient of the log-likelihood function is zero, and the eigenvalues of the Hessian matrix are all negative. In a local minimum, the eigenvalues of the Hessian matrix are all positive, whereas in a saddle point, some of them are positive and some negative. Nearly numerically singular Hessian matrices occur when the surface of the log-likelihood function is very flat about the estimate in some directions. This particularly happens when the model contains overly large degrees of freedom parameter estimates or the mixing weights $\alpha_{m,t}$ are estimated close to zero for all $t=1,...,T$ for some regime $m$.

\pkg{gmvarkit} provides several functions for evaluating whether the estimate is a local maximum point. The function \code{get_foc} returns the (numerically approximated) gradient of the log-likelihood function evaluated at the estimate, and the function \code{get_soc} returns eigenvalues of the (numerically approximated) Hessian matrix of the log-likelihood function evaluated at the estimate. The numerical derivatives are calculated using a central difference approximation
\begin{equation}
\frac{\partial L(\boldsymbol{\theta})}{\partial \theta_i} \approx \frac{f(\boldsymbol{\theta} + \boldsymbol{h}^{(i)}) - f(\boldsymbol{\theta} - \boldsymbol{h}^{(i)})}{2h}, \ \ h>0,
\end{equation}
where $\theta_i$ is the $i$th element of $\boldsymbol{\theta}$ and $\boldsymbol{h}^{(i)}=(0,...,0,h,0,...,0)$
contains $h$ as its $i$th element. By default, the difference $h=6\cdot 10^{-6}$ is used for all parameters except for overly large degrees of freedom parameters, whose partial derivatives are approximated using larger differences. The difference is increased for large degrees of freedom parameters, because the limited precision of the float point presentation induces artificially rugged surfaces to the their profile log-likelihood functions, and the increased differences diminish the related numerical error. On the other hand, as the surface of the profile log-likelihood function is very flat about a large degrees of freedom parameter estimate, large differences work well for the approximation.

For example, the following code calculates the first order condition for the G-StMVAR model \code{fit12gs}:
%
\begin{CodeChunk}
\begin{CodeInput}
R> get_foc(fit12gs)
\end{CodeInput}
\begin{CodeOutput}
 [1]  1.475392e-03  2.520058e-03  1.593868e-03 -4.325443e-03
 [5]  3.426033e-03  1.272343e-03 -7.762371e-04 -4.553748e-03
 [9]  3.181280e-03  3.919771e-03 -2.321276e-02  4.170381e-03
[13] -1.782989e-02  3.166647e-03 -3.229744e-03 -3.738130e-03
[17] -9.082465e-03  2.232360e-02 -7.895506e-03  9.746278e-06
\end{CodeOutput}
\end{CodeChunk}
%
and the following code calculates the second order condition:
%
\begin{CodeChunk}
\begin{CodeInput}
R> get_soc(fit12gs)
\end{CodeInput}
\begin{CodeOutput}
 [1] -1.329154e-01 -1.389172e+00 -1.273009e+01 -1.865806e+01
 [5] -2.067262e+01 -5.037907e+01 -8.094178e+01 -1.070554e+02
 [9] -1.715455e+02 -2.124879e+02 -2.769413e+02 -3.371079e+02
[13] -4.467047e+02 -1.104002e+03 -1.130339e+03 -1.261037e+03
[17] -1.820865e+03 -9.262083e+03 -1.302900e+04 -3.456585e+04
\end{CodeOutput}
\end{CodeChunk}
%
All eigenvalues of the Hessian matrix are negative, which points to a local maximum, but the gradient of the log-likelihood function seems to somewhat deviate from zero. The gradient might be inaccurate, because it is based on a numerical approximation. It is also possible that the estimate is inaccurate, because it is based on approximative numerical estimation, and the estimates are therefore not expected to be exactly accurate. Whether the estimate is a local maximum point with accuracy that is reasonable enough, can be evaluated by plotting the graphs of the profile log-likelihood functions about the estimate. In \pkg{gmvarkit}, this can be done conveniently with the function \code{profile_logliks}.

The exemplify, the following command plots the graphs of profile log-likelihood functions of the estimated G-StMVAR model \code{fit12gs}:
%
\begin{CodeChunk}
\begin{CodeInput}
R> profile_logliks(fit12gs, scale=0.02, precision=200)
\end{CodeInput}
\end{CodeChunk}
%
The resulting plot is presented in Figure~\ref{fig:proflogliks}.

%\begin{figure}%[p]
%  \centering
%  \includegraphics{figures/proflogliks.png}
%  \caption{The figure produced by the command \code{profile\_logliks(fit12gs, scale=0.02, precision=200)}. The graphs of the profile log-likelihood functions of the G-StMVAR model drawn about the estimate. The red vertical lines denote the estimate.}
%\label{fig:proflogliks}
%\end{figure}

The output shows that the estimate's accuracy is reasonable, as changing any individual parameter value marginally would not increase the log-likelihood much. The argument \code{scale} can be adjusted to shorten or lengthen the interval shown in the horizontal axis. If one zooms in enough by setting \code{scale} to a very small number, it can be seen that the estimate is not exactly at the local maximum, but it is so close that moving there would not increase the log-likelihood notably. The argument \code{precision} can be adjusted to increase the number of points the graph is based on. For faster plotting, it can be decreased, and for more precision, it can be increased. The argument \code{which_pars} is used to specify the parameters whose profile log-likelihood functions should be plotted. This argument is particularly useful when creating as many plots as there are parameters in the model to a single figure would cause the individual plots to be very small. In such a case, profile log-likelihood functions for subsets of the parameters can be plotted separately by specifying this argument accordingly.

We have discussed tools that can be utilized to evaluate whether the found estimate is a local maximum with a reasonable accuracy. It is, however, more difficult to establish that the estimate is the global maximum. With \pkg{gmvarkit}, the best way to increase the reliability that the found estimate is the global maximum, is to run more estimation rounds by adjusting the argument \code{ncalls} of the estimation function \code{fitGSMVAR}.

If the model is very large, a very large number of estimation rounds may be required to find the global maximum. If there are two regimes in the model, $p$ is reasonable, and the dimension of the time series at most four, the required number of estimation rounds typically varies from several hundred to several thousand depending on the model and the data. In the simpler models, less estimation rounds are required. In the larger models, and in particular if $M>2$ or $d>4$, a significantly large number of estimation rounds may be required obtain the MLE. Another thing that makes the estimation more challenging, are exotic parameter constraints that do not reduce the dimension of the parameter much. Constraints that greatly reduce complexity of the parameter space (such as constraining the autoregressive matrices to be identical in all regimes\footnote{Models constrained in this way can often be reliably estimated with a reasonable number of estimation rounds even when $M>2$}), on the other hand, make the estimation easier and reliable estimation of such models thereby require less estimation rounds.


\subsection{Estimation of the structural GSMVAR model}\label{sec:estim_structural}
As explained, \pkg{gmvarkit} currently supports two types of structural models: structural models identified recursively by the lower triangular Cholesky decomposition and structural models identified by conditional heteroskedasticity. Recursive identification is assumed for reduced form models, and thus do not require any further estimation.  Generalized impulse response functions and generalized forecast error variance decompositions can be estimated for recursively identified models by using the reduced form models directly in the functions \code{GIRF} and \code{GFEVD}. The rest of this section discusses estimation of structural models identified by conditional heteroskedasticity as in \cite{Virolainen2:2021}, which employ a parametrization different to the reduced form models.

The structural GSMVAR models are estimated similarly to the reduced form version, expect that the model is parametrized with $W$ and $\lambda_{mi}$, $m=2,...,M$, $i=1,...,d$ instead of the covariance matrices $\Omega_{m}$, $m=1,...,M$. The estimation is can be done with the function \code{fitGSMVAR} but now the argument \code{structural_pars} needs to be supplied with a list providing the constraints on $W$ (which equally imposes the constraints on the B-matrix), and optionally, linear constraints on the $\lambda_{mi}$ parameters or constraints restricting $\lambda_{mi}$ to fixed values.

The list \code{structural_pars} should contain at least the element \code{W} which is a $(dxd)$ matrix matrix with its entries imposing constraints on $W$: \code{NA} indicating that the element is unconstrained, a positive value indicating strict positive sign constraint, a negative value indicating strict negative sign constraint, and zero indicating that the element is constrained to zero. The elements named \code{C_lambda} and \code{fixed_lambdas} are optional (and alternative to each other).

If \code{C_lambda} is specified, it should be a $(d(M-1) \times r)$ constraint matrix that satisfies ($\lambda_{2},...,\lambda_{M}) =C_{\lambda} \gamma$ where $\lambda_{m}=(\lambda_{m1},...,\lambda_{md})$ and $\gamma$ is the new $(r x 1)$ parameter subject to which the model is estimated (similarly to AR parameter constraints). The entries of \code{C_lambda} must be either positive or zero.  Ignore (or set to \code{NULL}) if the eigenvalues $\lambda_{mi}$ should not be constrained. Note that other constraints than constraining some of the $\lambda_{mi}$ to be identical are not recommended but if such constraints are imposed, the argument \code{lambda_scale} in the genetic algorithm (see \code{?GAfit}) should be adjusted accordingly. If some of the $\lambda_{mi}$ are constrained to be identical, make sure the appropriate zero constraints placed in the $W$ matrix, because otherwise the MLE does not identify and you probably won't obtain any useful estimates \citep[see][Proposition 2]{Virolainen2:2021}.

If \code{fixed_lambdas} is specified, it should be a $d(M-1)$ length numeric vector $(\lambda_{2},...,\lambda_{M})$ specifying fixed values for the eigenvalue parameters $\lambda_{mi}$. They should be strictly larger than zero. Ignore (or set to \code{NULL}) if the eigenvalues $\lambda_{mi}$ should not be constrained. Note that you cannot use both \code{C_lambda} and \code{fixed_lambdas} at the same time.

\textbf{Reliable estimation of structural GSMVAR models typically requires much more estimation rounds than the estimation of the reduced form models. However, when $M=2$, every reduced form model has an implied statistically identified structural model, which can be built without any additional estimation (this will be discussed next). We recommend considering this implied model first. Then, if overidentifying constraints are to be imposed on the B-matrix (or equally $W$), we recommend using the unrestricted estimate to create an initial guess for the constrained parameter vector and pass this to the genetic algorithm as an initial population}. See the help page \code{?GAfit} for the arguments that can be passed by \code{fitGSMVAR} to the genetic algorithm. Create the initial guess for the parameter vector by using the form given in documentation of the argument \code{initpop}. If $M\neq 2$, the structural model needs to be estimated in the normal way with \code{fitGSMVAR}, however.

\subsubsection{Building structural model based on a reduced form model}
If the number of regimes is two ($M=2$), a structural model can be built based on a reduced form model, because the matrix decomposition used in the simultaneous diagonalization of the error term covariance matrices always exists. This can be done with function \code{gsmvar_to_sgsmvar} which should be supplied with the reduced form model, and it then returns a corresponding structural model. After creating the structural model, the columns of $W$ can be reordered with the function \code{reorder_W_columns} which also reorders all $\lambda_{mi}$ accordingly (and hence the resulting model will coincide with the original reduced form model). Also, all signs any column of $W$ can be swapped with the function \code{swap_W_signs}.

The exemplify, the following code creates statistically identified structural model based on the reduced form model \code{fit12gs} and then prints the estimates.
%
\begin{CodeChunk}
\begin{CodeInput}
R> fit12gss <- gsmvar_to_sgsmvar(fit12gs)
R> fit12gss
\end{CodeInput}
\begin{CodeOutput}
Structural G-StMVAR model:
 p = 1, M1 = 1, M2 = 1, d = 2, #parameters = 20, #observations = 244 x 2,
 conditional log-likelihood, intercept parametrization, no AR parameter
 constraints

Regime 1 (GMVAR type)
Mixing weight: 0.17
Regime means: 0.66, 1.67

   Y     phi0          A1                  Omega         1/2
1 y1 = [ 1.60 ] + [  0.13 -0.61 ] y1.1 + [  1.21 -0.04 ]     eps1
2 y2   [ 0.48 ]   [ -0.03  0.72 ] y2.1   [ -0.04  0.14 ]     eps2

Regime 2 (StMVAR type)
Mixing weight: 0.83
Regime means: 0.78, 0.54
Df parameter:  7.57

   Y     phi0          A1                            Omega          1/2
1 y1 = [ 0.55 ] + [  0.33 -0.04 ] y1.1 + (         [  0.42 0.00 ] )     eps1
2 y2   [ 0.12 ]   [  0.05  0.71 ] y2.1   ( ARCH_mt [  0.00 0.04 ] )     eps2

Structural parameters:
        W             lamb2
1 [  0.95 -0.55 ]   [  0.36 ]
2 [  0.16  0.34 ] , [  0.28 ]

The B-matrix (or equally W) is subject to 0 zero constraints and 2 sign
constraints. The eigenvalues lambda_{mi} are not subject to linear constraints.
\end{CodeOutput}
\end{CodeChunk}
%
Estimates for the structural parameters, $W$ and the eigenvalues, are printed last.

If there is only one mixture component, i.e., \code{M == 1}, \code{gsmvar_to_sgsmvar} returns a symmetric and pos. def. square root matrix of the error term covariance matrix by default. But one may also employ lower triangular Cholesky identification by setting \code{cholesky = TRUE} in the arguments.

\subsubsection{Estimating overidentified structural GSMVAR models}
Sometimes is appropriate to impose overidentifying constraints on $W$ (or equally the B-matrix). With preliminary estimates from the just-identified model in the case $M>1$ or any model in the case $M=1$, it is convenient to use the function \code{estimate_sgsmvar}. It takes in a reduced form or structural GSMVAR model as a class 'gsmvar' object and new constraints in the argument \code{new_W} as a matrix expressing the sign or zero constraints. Strictly positive or negative elements signify strict sign constraints, zeros zero constraints, and \code{NA} values that the element is unconstrained.

\code{estimate_sgsmvar} then creates preliminary estimate based on the supplied model and the constraints, and then runs the two-phase estimation with settings of the genetic algorithm such that the search is focused on the neighbourhood of the preliminary estimate. Thus, it will lead to the correct ML estimate only if the unconstrained estimate is close to the constrained one in the first place. It is therefore useful for imposing zero constraints for elements that are close to zero in the unrestricted estimate, for instance.

\textbf{It is important to make sure that supplied model readily satisfies the sign constraints that are imposed. To achieve this, you can swap the signs in each column of the W matrix with the function \code{swap\_W\_signs}. If the sign constraints are not not readily satisfie, the preliminary estimate switches the signs and will probably lead to incorrect estimate.}

\code{estimate_sgsmvar} can also be used to estimate models that are not identified, i.e., one regime models. If it supplied with a reduced form model, it will first apply the function \code{gsmvar_to_sgsmvar}, then impose the constraints and finally estimate the model.

\subsection{Constrained estimation}\label{sec:examp_const}

\subsubsection{Linear constraints on the autoregressive parameters}
Imposing linear constraints on the autoregressive parameters of GMVAR model is straightforward in \pkg{gmvarkit}. The constraints are expressed in a somewhat general form which allows to impose a wide class of constraints but one needs to take the time to construct the constraint matrix carefully for each particular case.

We consider constraints of form
\begin{align}
& (\boldsymbol{\phi}_1,...,\boldsymbol{\phi}_M) = \boldsymbol{C}\boldsymbol{\psi},\\
& \boldsymbol{\phi}_m=(vec(A_{m,1}),...,vec(A_{m,p}))\enspace (pd^2x1), \enspace m=1,...,M,
\end{align}
$\boldsymbol{C}$ is known $(Mpd^2xq)$ constraint matrix (of full column rank) and $\boldsymbol{\psi}$ is unknown $(qx1)$ parameter vector.

The parameter vector for constrained model has the size $((M(d+d(d+1)/2+1)+q-1)x1)$ and the form
\begin{equation}
\boldsymbol{\theta} = (\phi_{1,0},...,\phi_{M,0},\boldsymbol{\psi},\alpha_1,...,\alpha_{M-1},\boldsymbol{\nu}),
\end{equation}
where $\boldsymbol{\psi}$ is the $(qx1)$ parameter vector containing constrained autoregressive parameters. As in the case of regular models, instead of the intercept parametrization that takes use of intercept terms $\phi_{m,0}$, one may use the mean parametrization with regimewise means $\mu_m$ instead $(m=1,...,M)$.

\subsubsection{Examples of linear constraints}
Consider the following two common uses of linear constraints: restricting the autoregressive matrices to be the same for all regimes and constraining some AR parameters to zero. Of course also some other constraints may be useful, but we chose to show illustrative examples of these two, as they are taken use of in \cite{Kalliovirta+Meitz+Saikkonen:2016}.

\subsubsection{Restricting AR matrices to be the same for all regimes}
To restrict the AR matrices to be the same for all regimes, we want $\boldsymbol{\phi}_m$ to be the same for all $m=1,...,M$. The parameter vector $\boldsymbol{\psi}$ $(qx1)$ then corresponds to any $\boldsymbol{\phi}_m=\boldsymbol{\phi}$, and therefore $q=pd^2$. For the constraint matrix we choose
\begin{equation}
\boldsymbol{C} = [I_{pd^2}:\cdots:I_{pd^2}]' \enspace (Mpd^2xpd^2),
\end{equation}
that is, $M$ pieces of $(pd^2xpd^2)$ diagonal matrices stacked on top of each other, because then
\begin{equation}
\boldsymbol{C}\boldsymbol{\psi}=(\boldsymbol{\psi},...,\boldsymbol{\psi})=(\boldsymbol{\phi},...,\boldsymbol{\phi}).
\end{equation}
For instance, if there are two regimes in the model, the appropriate constraint matrix then created with \proglang{R} as
%
\begin{CodeChunk}
\begin{CodeInput}
R> p <- 1 # Any autoregressive order
R> d <- 2 # Whatever the dimension of the time series is
R> I_pd2 <- diag(p*d^2) # The appropriate diagonal matrix
R> (C1 <- rbind(I_pd2, I_pd2)) # Stack them on top of each other
\end{CodeInput}
\begin{CodeOutput}
     [,1] [,2] [,3] [,4]
[1,]    1    0    0    0
[2,]    0    1    0    0
[3,]    0    0    1    0
[4,]    0    0    0    1
[5,]    1    0    0    0
[6,]    0    1    0    0
[7,]    0    0    1    0
[8,]    0    0    0    1
\end{CodeOutput}
\end{CodeChunk}
%
The command \code{fitGSMVAR(gdpdef, p=1, M=2, model="GMVAR", constraints=C1)} would then estimate a GMVAR($1,2$) model with the AR matrices constrained to be the same in both regimes. In practice, you might want to adjust the number of CPU cores used, the of estimation rounds, and set seeds. Notably, with the dimension of the time series being only two and $p=1$ with two regimes, almost all of the estimation rounds end up in the MLE. Also, because model has the AR matrices constrained to be the same for all regimes, the estimation is much easier than with freely estimated models.

\subsubsection{Restricting AR parameters to be the same for all regimes and constraining non-diagonal elements of coefficient matrices to be zero}
The previous example shows how to restrict the AR parameters to be the same for all regimes, but say we also want to constrain the non-diagonal elements of coefficient matrices $A_{m,i}$ $(m=1,...,M, i=1,...,p)$ to be zero. We have the constrained parameter $\boldsymbol{\psi}$ $(qx1)$ representing the unconstrained parameters $(\boldsymbol{\phi_1},...,\boldsymbol{\phi}_M)$, where by assumption $\boldsymbol{\phi}_m=\boldsymbol{\phi}=(vec(A_1),...,vec(A_p))$ $(pd^2x1)$ and the elements of $vec(A_i)$ $(i=1,...,p)$ corresponding to the diagonal are zero.

For illustrative purposes, let's consider a GMVAR model with autoregressive degree $p=2$, number of mixture components $M=2$ and number of time series in the system $d=2$. Then we have
\begin{align}
\boldsymbol{\phi}&=(A_1(1,1),0,0,A_1(2,2),A_2(1,1),0,0,A_2(2,2)) \enspace (8x1) \enspace \text{and}\\
\boldsymbol{\psi}&=(A_1(1,1),A_1(2,2),A_2(1,1),A_2(2,2)) \enspace (4x1).
\end{align}
By a direct calculation, we can see that choosing the constraint matrix
\begin{equation}
\boldsymbol{C}=\left[{\begin{array}{c}
   \boldsymbol{\tilde{c}} \\
   \boldsymbol{\tilde{c}} \\
  \end{array}}\right]
\enspace (Mpd^2x4),
\enspace
\end{equation}
where
\begin{equation}
\boldsymbol{\tilde{c}}=\left[{\begin{array}{cccc}
   1 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0 \\
   0 & 1 & 0 & 0 \\
   0 & 0 & 1 & 0 \\
   0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 1 \\
  \end{array}}\right]
\enspace (pd^2x4)
\end{equation}
satisfies $\boldsymbol{C}\boldsymbol{\psi}=(\boldsymbol{\phi},...,\boldsymbol{\phi}).$

The above constraint matrix can be created with \proglang{R} as
%
\begin{CodeChunk}
\begin{CodeInput}
R> c_tilde <- matrix(0, nrow=2*2^2, ncol=4)
R> c_tilde[c(1, 12, 21, 32)] <- 1
R> C2 <- rbind(c_tilde, c_tilde)
R> C2
\end{CodeInput}
\begin{CodeOutput}
      [,1] [,2] [,3] [,4]
 [1,]    1    0    0    0
 [2,]    0    0    0    0
 [3,]    0    0    0    0
 [4,]    0    1    0    0
 [5,]    0    0    1    0
 [6,]    0    0    0    0
 [7,]    0    0    0    0
 [8,]    0    0    0    1
 [9,]    1    0    0    0
[10,]    0    0    0    0
[11,]    0    0    0    0
[12,]    0    1    0    0
[13,]    0    0    1    0
[14,]    0    0    0    0
[15,]    0    0    0    0
[16,]    0    0    0    1
\end{CodeOutput}
\end{CodeChunk}
%
The command \code{fitGSMVAR(gdpdef, p=2, M=2, model="GMVAR", constraints=C2)} would then estimate a GMVAR($2,2$) model with the AR matrices constrained to be the same in both regimes and the off-diagonal elements constrained to zero (again, you may want to adjust the arguments \code{ncalls}, \code{ncored}, and \code{seeds}).


\subsubsection{Constraining the unconditional means of some regimes to be the same}
In addition to constraining the autoregressive parameters, \pkg{gmvarkit} allows to constrain the unconditional means of some regimes to be the same. This feature is, however, only available for models that are parametrized with the unconditional means instead of intercepts (because some of the estimation is always done with mean-parametrization and one cannot generally swap the parametrization when constraints are imposed on means/intercepts). With the mean-parametrization employed (by setting \code{parametrization="mean"}), one may define groups of regimes that have the same mean parameters using the argument \code{same_means}. For instance, with three regime model $(M=3)$ the argument \code{same_means=list(c(1, 3), 2)} sets the unconditional means of the first and third regimes to be the same while allows the second regime to have different mean.

One can also combine linear constraints on the AR parameters with constraining some of the means to be the same. This allows, for instance, to estimate a model in which only the covariance matrix varies in time. To exemplify, the following code (which is not executed in this vignette) estimates a GMVAR($p=4, M=2$) model such that the unconditional means and autoregression matrices are constrained be the same in both regimes. The resulting model thereby has time-varying covariance matrix but otherwise it is linear.
%
\begin{CodeChunk}
\begin{CodeInput}
R> I_pd2 <- diag(4*2^2) # The appropriate diagonal matrix for the constraint matrix
R> C3 <- rbind(I_pd2, I_pd2) # Stack them on top of each other
R> fit42cm <- fitGSMVAR(gdpdef, p=4, M=2, model="GMVAR", parametrization="mean",
+    same_means=list(1:2), constraints=C3, ncalls=16, ncores=8, seeds=1:16)
\end{CodeInput}
\begin{CodeOutput}
Using 8 cores for 16 estimations rounds...
Optimizing with a genetic algorithm...
  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% elapsed=31s
Results from the genetic algorithm:
The lowest loglik:  -227.251
The mean loglik:    -225.743
The largest loglik: -224.648
Optimizing with a variable metric algorithm...
  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% elapsed=03s
Results from the variable metric algorithm:
The lowest loglik:  -223.311
The mean loglik:    -223.311
The largest loglik: -223.311
Calculating approximate standard errors...
Finished!
\end{CodeOutput}
\end{CodeChunk}
%

\subsubsection{Constraining the mixing weight parameters alphas to fixed values}
It is also possible to constrain the mixing weight parameters $\alpha_1,...,\alpha_{M-1}$ to some fixed values. To do so, specify the fixed values in the argument \code{weight_constraints} as a $(M-1\times 1)$ vector $(\alpha_1,...,\alpha_{M-1})$. Each element should be strictly between zero and one, and the sum of all of them should be strictly less than one. For instance, when $M=2$, specifying \code{weight_constraints=0.6} constraints the mixing weights parameters of the first regime as $\alpha_1=0.6$ (and hence $\alpha_2=0.4$).

\subsection{Testing parameter constraints}\label{sec:testconst}
One way to asses the validity of the imposed constraints is to compare the values of information criteria of the constrained and unconstrained models. \pkg{gmvarkit}, however, also provides functions for testing the constraints with the likelihood ratio test, Wald test, and Rao's test, which are applicable as the ML estimator of a GSMVAR model has the conventional asymptotic distribution \cite[as long as the model is correctly specified and one is willing to assume the validity of the required unverified assumptions; see][Theorem 3, and \cite{Kalliovirta+Meitz+Saikkonen:2016}, Theorem 3]{Virolainen2:2021}. For a discussion on the likelihood ratio and Wald tests, see \citet{Buse:1982} and the references therein, for example.

The likelihood ratio test considers the null hypothesis that the true parameter value $\boldsymbol{\theta}_0$ satisfies some constraints imposed on these parameters \cite[such that the constrained parameter space is a subset of the parameter space, which is presented in][Assumption 2 for the GSMVAR models]{Virolainen2:2021}. Denoting by $\hat{L}_U$ and $\hat{L}_C$ the (maximized) log-likelihoods based on the unconstrained and constrained ML estimates, respectively, the test statistic takes the form
\begin{equation}
LR=2(\hat{L}_U - \hat{L}_C).
\end{equation}
Under the null, the test statistic is asymptotically $\chi^2$-distributed with the degrees of freedom given by the difference in the dimensions of the unconstrained and constrained parameter spaces. With \pkg{gmvarkit}, the likelihood ratio test can be calculated with the function \code{LR_test}, which takes the unconstrained model (a class \code{gsmvar} object) as its first argument and the constrained model as the second argument.

\pkg{gmvarkit} implements the Wald test of the null hypothesis
\begin{equation}
A\boldsymbol{\theta}_0 = c,
\end{equation}
where $A$ is a $(k \times d)$ matrix with full row rank, $c$ is a $(k \times 1)$ vector, $\boldsymbol{\theta}_0$ is the true parameter value, $d$ is the dimension of the parameter space, and $k$ is the number of constraints. The Wald test statistic takes the form
\begin{equation}
W = (A\hat{\boldsymbol{\theta}} - c)' [A\mathcal{J}(\hat{\boldsymbol{\theta}})^{-1}A']^{-1}(A\hat{\boldsymbol{\theta}} - c),
\end{equation}
where $\mathcal{J}(\hat{\boldsymbol{\theta}})$ is the observed information matrix evaluated at the ML estimate $\hat{\boldsymbol{\theta}}$. Under the null, the test statistic is asymptotically $\chi^2$-distributed with $k$ degrees of freedom (which is the difference in dimensions of the constrained and unconstrained parameter spaces). With \pkg{gmvarkit}, the Wald test can be calculated with function \code{Wald_test}, which takes the estimated unconstrained model (as a class \code{gsmvar} object) as the first argument, the matrix $A$ as the second argument, and the vector $c$ as the third argument.

Rao's test is implemented to the function \code{Rao_test} (see function documentation on how to use it).

Note that the standard tests are not applicable if the number of GMVAR or StMVAR type regimes is chosen too large, as then some of the parameters are not identified, causing the result of the asymptotic normality of the ML estimator to break down. This particularly happens when one tests for the number of regimes in the model, as the under the null some of the regimes are reduced from the model\footnote{\cite{Meitz+Saikkonen:2021} have, however, recently developed such tests for mixture models with Gaussian conditional densities} \citep[see the related discussion in][]{Virolainen2:2021}. Similar caution applies for testing whether a regime is of the GMVAR type against the alternative that it is of the StMVAR type: then $\nu_m = \infty$ under the null for the regime $m$ to be tested, which violates the assumption that the parameter value is in the interior of a compact subset of the parameter space \citep[see][Theorem 3 and Assumption 1]{Virolainen2:2021}.


\section{Quantile residual based model diagnostics}\label{sec:qres}
In the GSMVAR models, the empirical counterparts of the error terms $\varepsilon_{m,t}$ in (\ref{eq:def}) cannot be calculated, because the regime that generated each observation is unknown, making the conventional residual based diagnostics unavailable. Therefore, \pkg{gmvarkit} utilizes so called quantile residuals, which are suitable for evaluating adequacy of the GSMAR models.

Denote by $y_t$, $t=1,2,...$,  the time series of interest and $\mathcal{F}_{t-1}$ the $\sigma$-algebra generated by the random variables or vectors $\lbrace y_{t-j}, j > 0 \rbrace$.  Moreover, let $\boldsymbol{\theta}$ denote the relevant parameter vector. \cite{Kalliovirta:2012} defines univariate quantile residuals as
\begin{equation}
R_{t,\boldsymbol{\theta}}=\Phi^{-1}(F(y_t;\boldsymbol{\theta}\mid \mathcal{F}_{t-1})),
\end{equation}
where $\Phi(\cdot)^{-1}$ is the standard normal quantile function and $F(\cdot\mid \mathcal{F}_{t-1})$ is the conditional distribution function of the considered model.

\cite{Kalliovirta+Saikkonen:2010} define multivariate quantile residuals analogously to the univariate ones but by taking into account the dependence of the component time series from each other.  Denote $\mathcal{A}_{j-1}=\sigma(y_{1,t},...,y_{j-1,t})$ and by $f(\cdot|\sigma(\mathcal{F}_{t-1},\mathcal{A}_{j-1}))=f_{j-1,t-1}(\cdot)$ the conditional density function conditional on the $\sigma$-algebra $\sigma(\mathcal{F}_{t-1},\mathcal{A}_{j-1})$

The conditional density function of the random vector $y_t$ can be expressed in a product form by conditioning to the components $y_t$ in addition to the history $\mathcal{F}_{t-1}$ as
\begin{equation}\label{eq:prodform}
f(y_t;\theta|\mathcal{F}_{t-1})=\prod_{j=1}^{d}f_{j-1,t-1}(y_{j,t};\boldsymbol{\theta}),
\end{equation}
where $y_{j,t}$ is the $j$th component of $y_t$ and $f_{0,t-1}(y_{1,t};\boldsymbol{\theta})=f_{1,t-1}(y_{1,t};\boldsymbol{\theta})$ is the marginal conditional density function of $y_{1,t}$ conditional on $\mathcal{F}_{t-1}$.

The conditional distribution functions corresponding to the density functions $f_{j-1,t-1}(\cdot;\boldsymbol{\theta})$ in (\ref{eq:prodform}) are of the form
\begin{equation}
F_{j-1,t-1}(y_{j,t};\boldsymbol{\theta})=\int_{-\infty}^{y_{j,t}} f_{j-1,t-1}(u;\boldsymbol{\theta})du.
\end{equation}

The multivariate quantile residuals are then defined as
\begin{equation}\label{eq:qrdef}
R_{t,\boldsymbol{\theta}}=
\begin{bmatrix}
  R_{1t,\boldsymbol{\theta}} \\
  R_{2t,\boldsymbol{\theta}} \\
  \vdots \\
  R_{dt,\boldsymbol{\theta}} \\
\end{bmatrix} =
\begin{bmatrix}
  \Phi^{-1}(F_{0,t-1}(y_{1,t};\boldsymbol{\theta})) \\
  \Phi^{-1}(F_{1,t-1}(y_{2,t};\boldsymbol{\theta}))\\
  \vdots \\
  \Phi^{-1}(F_{d-1,t-1}(y_{d,t};\boldsymbol{\theta})) \\
\end{bmatrix},
\end{equation}
and its empirical counterpart, $r_{t,\hat{\boldsymbol{\theta}}}$,  is obtained by replacing the parameter $\boldsymbol{\theta}$ with its maximum likelihood estimate $\hat{\boldsymbol{\theta}}$. Closed form expressions for the quantile residuals of the G-StMVAR model (which encompasses GMVAR and StMVAR models as special cases) are derived in Appendix~\ref{ap:qresexpr}

For a correctly specified GSMVAR model employing the ML estimator, the empirical counterparts of multivariate quantile residuals are asymptotically multivariate standard normal \citep[Lemma 3]{Kalliovirta+Saikkonen:2010}. They can therefore be utilized in graphical diagnostic simalarly to the conventional Pearson's residuals. For the graphical diagnostics, \pkg{gmvarkit} provides function \code{diagnostic_plot} which plots the quantile residual time series, auto- and crosscorrelations of the quantile residuals, auto- and crosscorrelations of the squared quantile residuals, and normal quantile-quantile plots as well as histrograms of the quantile residuals.

\cite{Kalliovirta+Saikkonen:2010} also propose three diagnostic tests for testing normality, autocorrelation, and conditional heteroskedasticity of the quantile residuals. The tests can be based either on the data or on a simulation procedure. If the sample is short, tests based on the data can be too forgiving, so to obtain more reliable test results the simulation procedure is recommended (with sample size of at least several thousand). The tests can be calculated with \pkg{gmvarkit} by using the function \code{quantile_residual_tests}. The simulation procedure is employed if the argument \code{nsim} is set larger the number of observations (in each component time series). In this case, \code{nsim} sets the length of the sample path used in the simulation procedure. If one is concerned about autocorrelation or conditional heteroskedasticity in a specigic lag, the (standardized) individual statistics discussed in \cite{Kalliovirta+Saikkonen:2010} can be examined. The function \code{quantile_residual_tests} returns them automatically for the specified lags.


\section{Impulse response analysis}\label{sec:impulseresponse}

\subsection{Generalized impulse response function}
We consider the generalized impulse response function (GIRF) \citep{Koop+Pesaran+Potter:1996} defined as
\begin{equation}\label{eq:girf}
\text{GIRF}(n,\delta_j,\mathcal{F}_{t-1}) = \text{E}[y_{t+n}|\delta_j,\mathcal{F}_{t-1}] - \text{E}[y_{t+n}|\mathcal{F}_{t-1}],
\end{equation}
where $n$ is the chosen horizon, $\mathcal{F}_{t-1}=\sigma\lbrace y_{t-j},j>0\rbrace$ as before, the first term in the RHS is the expected realization of the process at time $t+n$ conditionally on a structural shock of magnitude $\delta_j \in\mathbb{R}$ in the $j$th element of $e_t$ at time $t$ and the previous observations, and the second term in the RHS is the expected realization of the process conditionally on the previous observations only. GIRF thus expresses the expected difference in the future outcomes when the specific structural shock hits the system at time $t$ as opposed to all shocks being random.

Due to the $p$-step Markov property of the GSMVAR model, conditioning on (the $\sigma$-algebra generated by) the $p$ previous observations $\boldsymbol{y}_{t-1}\equiv(y_{t-1},...,y_{t-p})$ is effectively the same as conditioning on $\mathcal{F}_{t-1}$ at the time $t$ and later. The history $\boldsymbol{y}_{t-1}$ can be either fixed or random, but with random history the GIRF becomes a random vector, however. Using fixed $\boldsymbol{y}_{t-1}$ makes sense when one is interested in the effects of the shock in a particular point of time, whereas more general results are obtained by assuming that $\boldsymbol{y}_{t-1}$ follows the stationary distribution of the process. If one is, on the other hand, concerned about a specific regime, $\boldsymbol{y}_{t-1}$ can be assumed to follow the stationary distribution of the corresponding component model.

In practice, the GIRF and its distributional properties can be approximated with a Monte Carlo algorithm that generates independent realizations of the process and then takes the sample mean for point estimate. If $\boldsymbol{y}_{t-1}$ is random and follows the distribution $G$, the GIRF should be estimated for different values of $\boldsymbol{y}_{t-1}$ generated from $G$, and then the sample mean and sample quantiles can be taken to obtain the point estimate and confidence intervals. The algorithm implemented in \pkg{gmvarkit} is presented in Appendix~\ref{sec:montecarlo}.

Because the GSMVAR model allows to associate specific features or economic interpretations for different regimes, it might be interesting to also examine the effects of a structural shock to the mixing weights $\alpha_{m,t}$, $m=1,...,M$. We then consider the related GIRFs $E[\alpha_{m,t+n}|\delta_j,\boldsymbol{y}_{t-1}] - E[\alpha_{m,t+n}|\boldsymbol{y}_{t-1}]$ for which point estimates and confidence intervals can be constructed similarly to (\ref{eq:girf}).

In \pkg{gmvarkit}, the GIRF can be estimated with the function \code{GIRF} which should be supplied with the estimated GSMVAR model or a GSMVAR built with hand-specified parameter values using the function \code{GSMVAR}. If a reduced form model is supplied to \code{GIRF}, recursive identification by lower-triangular Cholesky decomposition is automatically assumed. The size of the structural shock can be set with the argument \code{shock_size}. If not specified, the size of one standard deviation is used; that is, the size one. Among other arguments, the function may also be supplied with the argument \code{init_regimes} which specifies from which regimes' stationary distributions the initial values are generated from (if more than one regime is specified, the initial values will be generated from a mixture of the stationary distributions with the relative mixing proportions given by the mixing weight parameters). If more than one regime is specified, a mixture distribution with weights given by the mixing weight parameters is used. Alternatively, one may specify fixed initial values with the argument \code{init_values}. Note that the confidence intervals (whose confidence level can be specified with the argument \code{ci}) reflect uncertainty about the initial value only and not about the parameter estimates.

Due to the nonlinear nature of the model, GIRFs estimated from different starting values, or with different sign or magnitude of the shock, generally move the variables differently. Sometimes it is, however, of interest to scale the impulse responses so that they correspond to movement of some specific sign and magnitude of some specific variable. In \pkg{gmvarkit}, this is most conveniently achieved with the arguments \code{scale} and \code{scale_type}. The argument \code{scale} can be specified in order to scale the GIRFs to some of the shocks so that they correspond to a specific magnitude of instantaneous or peak response of some specific variable. For a single shock, it should a length three vector where the shock of interest is given in the first element (an integer in $1,...,d$), the variable according to which the GIRFs should be scaled in the second element (an integer in $1,...,d$), and the magnitude of the given variable's instantaneous or peak response in the third element (a non-zero real number). If the GIRFs of multiple shocks should be scaled, provide a matrix which has one column for each of the shocks with the columns being the length three vectors described above. The argument \code{scale_type} should be either \code{"instant"} or \code{"peak"} specifying whether you want to scale according to the instantaneous movement of peak response. If \code{"peak"}, the scale is based on the largest magnitude of peak response in absolute value. Scaling according to peak response won't based on values after the horizon specified in the argument \code{"scale_horizon"}. Note that if you scale the GIRFs, the scaled GIRFs of mixing weights can be outside the interval from zero to one.

Because estimating the GIRF and their confidence intervals is computationally demanding, parallel computing is taken use of to shorten the estimation time. The number of CPU cores used can be set with the argument \code{ncores}. The objects returned by the \code{GIRF} function have their own \code{plot} and \code{print} methods. Also, cumulative impulse responses of the specified variables can be obtained directly by specifying the argument \code{which_cumulative}.

\subsection{Generalized forecast error variance decomposition}
We consider the generalized forecast error variance decomposition (GFEVD) \citep{Lanne+Nyberg:2016}  that is defined for variable $i$, shock $j$, and horizon $n$ as
\begin{equation}
\text{GFEVD}(n,y_{it}, \delta_j,\mathcal{F}_{t-1}) = \frac{\sum_{l=0}^n\text{GIRF}(l,\delta_j,\mathcal{F}_{t-1})_i^2}{\sum_{k=1}^d\sum_{l=0}^n\text{GIRF}(l,\delta_k,\mathcal{F}_{t-1})_i^2},
\end{equation}
where $n$ is the chosen and $\text{GIRF}(l,\delta_j,\mathcal{F}_{t-1})_i$ is the $i$th element of the related GIRF (see also the notation described for GIRF in the previous section). That is, the GFEVD is otherwise similar to the conventional forecast error variance decomposition but with GIRFs in the place of conventional impulse response functions. Because the GFEVDs sum to unity (for each variable), they can be interpreted in a similar manner to the conventional FEVD.

In \pkg{gmvarkit}, the GFEVD can be estimated with the function \code{GFEVD}. As with the GIRF, the GFEVD is dependent on the initial values. The type of the initial values is set with the argument \code{initval_type}, and there are three options:
\begin{enumerate}
\item \code{"data"} which estimates the GIRFs for all possible length $p$ histories in the data and then the GIRFs in the GFEVD are obtained as the sample mean over those GIRFs.
\item \code{"random"} which generates the initial values from the stationary distribution of the process or from the mixture of the stationary distributions of some specific regime(s) with the relative mixing proportions given by the mixing weight parameters. The initial regimes can be set with the argument \code{init_regimes}. The GIRFs in the GFEVD are obtained as the sample mean over the GIRFs estimated for the different random initial values.
\item \code{"fixed"} which estimates the GIRFs for a single fixed initial value that is set with the argument \code{init_values}.
\end{enumerate}
The shock size is the same for all scalar components of the structural shock and it can be adjusted with the argument \code{shock_size}. If the GIRFs for some variables should be cumulative before calculating the GFEVD, specify them with the argument \code{which_cumulative}. Finally, note that the GFEVD objects have their own plot and print methods.

\subsection{Linear impulse response functions}
It is also possible to calculate linear impulse response functions (IRF) based on a specific regime of the estimated model by using the function \code{linear_IRF}. If the autoregressive dynamics of the model are linear (i.e., either $M=1$ or mean and AR parameters are constrained identical across the regimes), confidence bounds can be estimated based on a type of a fixed-design wild residual bootstrap method. \code{gmvarkit} implements the method proposed \cite{Herwartz+Lutkepohl:2014}.

\section{Building a GSMVAR model with specific parameter values}\label{sec:GSMVAR}
The function \code{GSMVAR} facilitates building GSMVAR models without estimation, for instance, in order to simulate observations from a GSMVAR process with specific parameter values. The parameter vector (of length $M(d + d^2p + d(d+1)/2 + 2) - M_1 - 1$ for unconstrained reduced form models) has the form $\boldsymbol{\theta} = (\boldsymbol{\vartheta}_1,...,\boldsymbol{\vartheta}_M,\alpha_1,...,\alpha_{M-1},\boldsymbol{\nu})$ where
\begin{align}
\boldsymbol{\vartheta}_m &= (\varphi_{m,0},\text{vec}({A_{m,1}}),...,\text{vec}(A_{m,p}),\text{vech}(\Omega_m)),\ \ m=1,...,M, \ \text{and}  \\
\boldsymbol{\nu} &= (\nu_{M_1+1},...,\nu_M).
\end{align}
%
In the GMVAR model (when $M_1=M$), the vector $\boldsymbol{\nu}$ is omitted, as the GMVAR model does not contain degrees of freedom parameters. For models imposing additional constraints on the paremeters, the parameter vectors are expressed in a different way. They are only presented in the package documentation for brevity, because the hand-specified parameter values can be set to satisfy any constraints as is.

In a structural GSMVAR model, the parameter vector has the form
\begin{align}
\boldsymbol{\theta} &= (\varphi_{1,0},...,\varphi_{M,0},\text{vec}(\boldsymbol{A}_1),...,\text{vec}(\boldsymbol{A}_M),\text{vec}(W),\boldsymbol{\lambda}_1,...,\boldsymbol{\lambda}_M, \alpha_1,...,\alpha_{M-1},\boldsymbol{\nu}), \\
\boldsymbol{A}_m & = (\text{vec}({A_{m,1}}),...,\text{vec}(A_{m,p})) \\
\boldsymbol{\lambda}_m &= (\lambda_{m1},...,\lambda_{md}).
\end{align}
For constrained structural models (including constraints on the structural parameters), see the documentation of \code{GSMVAR} (or any other relevant function).

In addition to the parameter vector, \code{GSMVAR} should be supplied with arguments \code{p} and \code{M} specifying the order of the model similarly to the estimation function \code{fitGSMVAR} discussed in Section \ref{sec:example_estim}. If one wishes to parametrize the model with the regimewise unconditional means ($\mu_m$) instead of the intercepts ($\varphi_{m,0}$), the argument \code{parametrization} should be set to \code{"mean"} in which case the intercept parameters $\varphi_{m,0}$ are replaced with $\mu_m$ in the parameter vector. By default, \pkg{gmvarkit} uses intercept parametrization.

To exemplify, we build a reduced form StMVAR $p=1$, $M=1$, $d=2$ model. The model has intercept parametrization and parameter values $\varphi_{1,0}=(0, 1)$ $\text{vec}(A_{1,1}) = (0.2, 0.2, 0.2, -0.2)$, $\text{vech}(\Omega_2) = (1, 0.1, 1)$, and $\nu_1 = 3$. After building the model, we use the \code{print} method to examine it:
%
\begin{CodeChunk}
\begin{CodeInput}
R> params112 <- c(0, 1, 0.2, 0.2, 0.2, -0.2, 1, 0.1, 1, 3)
R> mod112 <- GSMVAR(p=1, M=1, d=2, params=params112, model="StMVAR")
R> mod112
\end{CodeInput}
\begin{CodeOutput}
Reduced form StMVAR model:
 p = 1, M = 1, d = 2, #parameters = 10,
 conditional log-likelihood, intercept parametrization, no AR parameter
 constraints

Regime 1
Mixing weight: 1.00
Regime means: 0.22, 0.87
Df parameter:  3.00

   Y     phi0          A1                            Omega          1/2
1 y1 = [ 0.00 ] + [  0.20  0.20 ] y1.1 + (         [  1.00 0.10 ] )     eps1
2 y2   [ 1.00 ]   [  0.20 -0.20 ] y2.1   ( ARCH_mt [  0.10 1.00 ] )     eps2
\end{CodeOutput}
\end{CodeChunk}
%

It is possible to include data in the models built with \code{GSMVAR} by either providing the data in the argument \code{data} when creating the model or by adding the data afterwards with the function \code{add_data}. When the model is supplied with data, the mixing weights, one-step conditional means and variances, and quantile residuals can be calculated and included in the model. The function \code{add_data} can also be used to update data to an estimated GSMVAR model without re-estimating the model.

\section{Simulation and forecasting}\label{sec:simufore}

\subsection{Simulation}\label{sec:simu}
\pkg{gmvarkit} implements the S3 method \code{simulate} for simulating observations from GSMVAR processes (see \code{?simulate.gsmvar}). The method requires the process to be given as a class \code{gsmvar} object, which are typically created either by estimating a model with the function \code{fitGSMVAR} or by specifying the parameter values by hand and building the model with the constructor function \code{GSMVAR}. The initial values required to simulate the first $p$ observations can be either set by hand (with the argument \code{init_values}) or drawn from the stationary distribution of the process (by default) or from a (mixture of) stationary distribution(s) of given regime(s). The argument \code{nsim} sets the length of the sample path to be simulated.

To give an example, the following code sets the random number generator seed to one and simulates $500$ observations long sample from the StMVAR process built in Section~\ref{sec:GSMVAR}:
%
\begin{CodeChunk}
\begin{CodeInput}
R> mysim <- simulate(mod112, nsim=500, seed=1)
\end{CodeInput}
\end{CodeChunk}
%
Our implementation of \code{simulate} returns a list containing the simulated sample path in \code{\$sample}, the mixture component that generated each observation in \code{\$component}, and the mixing weights in \code{\$mixing_weights}.

\subsection{Simulation based forecasting}
Deriving multiple-steps-ahead point predictions and prediction intervals analytically for the GSMVAR models is very complicated, so \pkg{gmvarkit} employs the following simulation-based method. By using the last $p$ observations of the data up to the date of forecasting as initial values, a large number of sample paths for the future values of the process are simulated. Then, sample quantiles from the simulated sample paths are calculated to obtain prediction intervals, and the median or mean is used for point predictions. A similar procedure is also applied to forecast future values of the mixing weights, which might be of interest because the researcher can often associate specific characteristics to different regimes.

Forecasting is most conveniently done with the \code{predict} method (see \code{?predict.gsmvar}). The available arguments include the number of steps ahead to be predicted (\code{n_ahead}), the number sample paths the forecast is based on (\code{nsim}), possibly multiple confidence levels for prediction intervals (\code{pi}), prediction type (\code{pred_type}), and prediction interval type (\code{pi_type}). The prediction type can be either \code{median}, \code{mean}, or for one-step-ahead forecasts also the exact conditional mean, \code{cond_mean}. The prediction interval type can be any of \code{"two-sided"}, \code{"upper"}, \code{"lower"}, or \code{"none"}.

To exemplify, the following code forecasts the two-dimensional time-series of U.S. GDP and GDP deflator growth using the G-StMVAR($1, 1, 1$) model \code{fit12gs} estimated in Section~\ref{sec:example_estim}. The forecast is $10$ steps (quarters in this case) ahead, based on $10000$ Monte Carlo repetitions with the point forecast based on the mean of those repetitions. The prediction intervals are two-sided with confidence levels $0.95$ and $0.90$. Finally, the argument \code{mix_weights} states that also future values of the the mixing weights should be forecasted. After completing the forecast, the function plots the results by default.
%
\begin{CodeChunk}
\begin{CodeInput}
R> mypred <- predict(fit12gs, n_ahead=10, nsim=10000, pred_type="mean",
+                    pi_type="two-sided", pi=c(0.95, 0.90),
+                    mix_weights=TRUE)
\end{CodeInput}
\end{CodeChunk}
%
The resulting plot is presented in Figure~\ref{fig:predplot}.


%\begin{figure}%[p]
%  \centering
%  \includegraphics{figures/predplot.png}
%  \caption{The figure produced by the predict method applied to the model \code{fit12gs}. The first top figures show the point and interval predictions $10$ steps ahaed for the time series, and the bottom figure for the mixing weights. Two-sided prediction intervals with confidence levels $95\%$ and $90\%$, point forecast based on mean. Prediction based on $10000$ Monte Carlo repetitions.}
%\label{fig:predplot}
%\end{figure}


\begin{table}
\centering
\begin{tabular}{llp{6.4cm}}
\hline
Related to     & Name                      & Description \\ \hline
Estimation     & \code{fitGSMVAR}          & Estimate a GSMVAR model.\\
               & \code{alt_gsmvar}         & Build a GSMVAR model based on results from any estimation round.\\
               & \code{stmvar_to_gstmvar}  & Estimate a G-StMVAR model based on a StMVAR (or G-StMVAR) model with large degrees of freedom parameters.\\
               & \code{gsmvar_to_sgsmvar}  & Obtain a structural GSMVAR model identified by heteroskedasticity based on a reduced form model.\\
               & \code{iterate_more}       & Run more iterations of the variable metric algorithm for a preliminary estimated GSMVAR model.\\
Estimates      & \code{summary} (method)   & Detailed printout of the estimates.\\
               & \code{plot} (method)      & Plot the series with the estimated mixing weights and a kernel density estimates of the (marginal) series with the (marginal) stationary densities of the model.\\
               & \code{get_foc}            & Calculate numerically approximated gradient of the log-likelihood function evaluated at the estimate.\\
               & \code{get_soc}            & Calculate eigenvalues of numerically approximated Hessian of the log-likelihood function evaluated at the estimate.\\
               & \code{profile_logliks}    & Plot the graphs of the profile log-likelihood functions.\\
               & \code{cond_moment_plot}   & Plot the model implied one-step conditional means or variances.\\
Diagnostics    & \code{quantile_residual_tests} & Calculate quantile residual tests.\\
               & \code{diagnostic_plot}    & Plot quantile residual diagnostics.\\
Forecasting    & \code{predict} (method)   & Forecast future observations and mixing weights of the process.\\
Simulation     & \code{simulate} (method)  & Simulate from a GSMVAR process.\\
Create model   & \code{GSMVAR}             & Construct a GSMVAR model based on specific parameter values.\\
Hypothesis testing & \code{LR_test}        & Calculate likelihood ratio test.\\
               & \code{Wald_test}          & Calculate Wald test.\\
               & \code{Rao_test}           & Calculate Rao's test.\\
Impulse response analysis & \code{GIRF}    & Estimate generalized impulse response functions.\\
               & \code{GFEVD}              & Estimate generalized forecast error variance decomposition.\\
               & \code{linear_IRF}         & Estimate linear impulse response functions.\\
Other          & \code{add_data}           & Add data to a GSMVAR model \\
               & \code{swap_parametrization} & Swap between mean and intercept parametrizations \\
\hline
\end{tabular}
\caption{Some useful functions in \pkg{gmvarkit} sorted according to their usage. The note "method" in parentheses after the name of a function signifies that it is an S3 method for a class \code{gsmvar} object (often generated by the function \code{fitGSMVAR} or \code{GSMVAR}).}
\label{tab:functions}
\end{table}

\section{Summary}\label{sec:summary}
Mixture vector autoregressive models are a valuable tool in modeling multivariate time series in which the data generating dynamics vary in time. We described the \proglang{R} package \pkg{gmvarkit}, which accommodates the GMVAR model \citep{Kalliovirta+Meitz+Saikkonen:2016}, the StMVAR model \citep{Virolainen2:2021}, and the G-StMVAR model \citep{Virolainen2:2021} - an appealing family of mixture vector autoregressive models that call the GSMVAR models. We discussed several features provided by \pkg{gmvarkit} for GSMVAR modeling: unconstrained and constrained maximum likelihood estimation of the model parameters, hypothesis testing, quantile residual based model diagnostics, estimation of generalized impulse response function and generalized forecast error variance decomposition, simulation, forecasting, and more. For convenience, we have collected some useful functions in \pkg{gmvarkit} to Table~\ref{tab:functions}. For all the exported functions and their usage, see the reference manual.


\section*{Computational details}
The results in this paper were obtained using \proglang{R}~4.1.2 and \pkg{uGMAR}~3.4.1 package running on MacBook Pro 14", 2021, with Apple M1 Pro processor, 16 Gt of unified RAM, and macOS Monterey 12.1 operating system.

Some of the estimation results (and thereby everything that is calculated based on the estimates) may vary slightly when running the code on different computers. This is likely due to the numerical error caused by the limited precision of the float point representation.

\pagebreak
\bibliography{refs}

\newpage

\begin{appendix}
\section{Properties of multivariate Gaussian and Student's $t$ distribution}\label{ap:propt}
Denote a $d$-dimensional real valued vector by $y$.  It is well known that the density function of a $d$-dimensional Gaussian distribution with mean $\mu$ and covariance matrix $\Sigma$ is
\begin{equation}
n_d(y;\mu,\Sigma) = (2\pi)^{-d/2}\text{det}(\Sigma)^{-1/2}\exp\left\lbrace -\frac{1}{2}(y -\mu)'\Sigma^{-1}(y - \mu) \right\rbrace .
\end{equation}
Similarly to \cite{Meitz+Preve+Saikkonen:2021} but differing from the standard form, we parametrize the Student's $t$-distribution using its covariance matrix as a parameter together with the mean and the degrees of freedom. The density function of such a $d$-dimensional $t$-distribution with mean $\mu$, covariance matrix $\Sigma$, and $\nu>2$ degrees of freedom is
\begin{equation}
t_d(y;\mu,\Sigma,\nu)=C_d(\nu)\text{det}(\Sigma)^{-1/2}\left(1+\frac{(y -\mu)'\Sigma^{-1}(y - \mu)}{\nu-2}\right)^{-(d+\nu)/2},
\end{equation}
where
\begin{equation}
C_d(\nu)=\frac{\Gamma\left(\frac{d+\nu}{2}\right)}{\sqrt{\pi^d(\nu-2)^d}\Gamma\left(\frac{\nu}{2}\right)},
\end{equation}
and $\Gamma\left(\cdot\right)$ is the gamma function.  We assume that the covariance matrix $\Sigma$ is positive definite for both distributions.

Consider a partition $X=(X_1,X_2)$ of either Gaussian or $t$-distributed (with $\nu$ degrees of freedom) random vector $X$ such that $X_1$ has dimension $(d_1\times1)$ and $X_2$ has dimension $(d_2\times1)$. Consider also a corresponding partition of the mean vector $\mu=(\mu_1,\mu_2)$ and the covariance matrix
\begin{equation}
\Sigma=
\begin{bmatrix}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{12}' & \Sigma_{22}
\end{bmatrix},
\end{equation}
where, for example, the dimension of $\Sigma_{11}$ is $(d_1\times d_1)$.  In the Gaussian case, $X_1$ then has the marginal distribution $n_{d_1}(\mu_1,\Sigma_{11})$ and $X_2$ has the marginal distribution $n_{d_2}(\mu_2,\Sigma_{22})$.  In the Student's $t$ case,  $X_1$ has the marginal distribution $t_{d_1}(\mu_1,\Sigma_{11},\nu)$ and $X_2$ has the marginal distribution $t_{d_2}(\mu_2,\Sigma_{22},\nu)$ (see, e.g., \cite{Ding:2016}, also in what follows).

When $X$ has Gaussian distribution,  the conditional distribution of the random vector $X_1$ given $X_2=x_2$ is
\begin{equation}
X_1\mid(X_2=x_2)\sim n_{d_1}(\mu_{1\mid2}(x_2),\Sigma_{1\mid2}(x_2)),
\end{equation}
where
\begin{align}
\mu (x_2)\equiv \mu_{1\mid2}(x_2) &= \mu_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2) \quad \text{and}\label{eq:mux_gaus} \\
\Omega \equiv \Sigma_{1\mid2}(x_2) &= \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}'. \label{eq:omegax_gaus}
\end{align}

When $X$ has $t$-distribution, the conditional distribution of the random vector $X_1$ given $X_2=x_2$ is
\begin{equation}
X_1\mid(X_2=x_2)\sim t_{d_1}(\mu_{1\mid2}(x_2),\Sigma_{1\mid2}(x_2),\nu+d_2),
\end{equation}
where
\begin{align}
\mu (x_2) = \mu_{1\mid2}(x_2) &= \mu_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2) \quad \text{and}\label{eq:mux} \\
\Omega (x_2) \equiv \Sigma_{1\mid2}(x_2) &= \frac{\nu-2+(x_2-\mu_2)'\Sigma_{22}^{-1}(x_2-\mu_2)}{\nu-2+d_2}(\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}'). \label{eq:omegax}
\end{align}
In particular, we have
\begin{align}\label{eq:td_decomp}
n_d(x;\mu,\Sigma) &= n_{d_1}(x_1;\mu_{1|2}(x_2),\Sigma_{1|2}(x_2))t_{d_2}(x_2;\mu_2,\Sigma_{22}) \quad \text{and}\\
t_d(x;\mu,\Sigma,\nu) &= t_{d_1}(x_1;\mu_{1|2}(x_2),\Sigma_{1|2}(x_2),\nu+d_2)t_{d_2}(x_2;\mu_2,\Sigma_{22},\nu).
\end{align}



\section{Quantile residuals of the G-StMVAR model}\label{ap:qresexpr}
The conditional density function of the $d$-dimensional G-StMVAR process $y_t$ conditional on  $\mathcal{F}_{t-1}$ is
\begin{equation}\label{eq:gstmvarconddens}
f_{t-1}(y_t;\boldsymbol{\theta})=\sum_{m=1}^{M_1}\alpha_{m,t}n_d(y_t;\mu_{m,t},\Omega_{m}) + \sum_{m=M_1+1}^{M}\alpha_{m,t}t_d(y_t;\mu_{m,t},\Omega_{m,t},\nu_m + dp),
\end{equation}
where $n_d(\cdot;\mu_{m,t},\Omega_m,\nu_m + dp)$ is the density function of $d$-dimensional normal distribution with mean $\mu_{m,t}$ and covariance matrix $\Omega_{m}$; and  $t_d(\cdot;\mu_{m,t},\Omega_m,\nu_m + dp)$ is the density function of $d$-dimensional $t$-distribution with mean $\mu_{m,t}$, covariance matrix $\Omega_{m,t}$,  and $\nu_m + dp$ degrees of freedom.

Denote $y_t^{(k)}=(y_{1,t},...,y_{k,t})$ $(k\times 1)$,  $k\leq d$,   $\mu_{m,t}^{(k)}=(\mu_{1,m,,t},...,\mu_{k,m,t})$ $(k\times 1)$,  $k\leq d$,  and by $\Omega_{m,t}^{(k)}$ ($\Omega_{m}^{(k)}$) the upper left $(k\times k)$ block matrix of $\Omega_{m,t}$ ($\Omega_{m}$).  Then,  the properties of the marginal distributions of multivariate Gaussian and $t$-distributions (see Appendix~\ref{ap:propt}) show that conditional on $\mathcal{F}_{t-1}$,  the random vectors $y_t^{(j)}$, $j=1,..,d$,  follow the distribution that is a mixture $M_1$ $j$-dimensional normal distributions (with means $\mu_{m,t}^{(j)}$ and covariance matrices $\Omega_{m}^{(j)}$) and $M_2\equiv M-M_1$ $j$-dimensional $t$-distributions (with means $\mu_{m,t}^{(j)}$,  covariance matrices $\Omega_{m,t}^{(j)}$, and $\nu_m + dp$ degrees of freedom).  The mixing weights $\alpha_{m,t}$ are not affected, as they are $\mathcal{F}_{t-1}$-measurable.  Therefore, the marginal density function of $y_t^{(j)}$ is
\begin{equation}\label{eq:ytj_margdens}
f_{t-1}(y_{t}^{(j)};\boldsymbol{\theta})=\sum_{m=1}^{M_1}\alpha_{m,t} n_j(y_{t}^{(j)};\mu_{m,t}^{(j)},\Omega_{m}^{(j)}) + \sum_{m=M_1 + 1}^{M}\alpha_{m,t} t_j(y_{t}^{(j)};\mu_{m,t}^{(j)},\Omega_{m,t}^{(j)},\nu_m + dp),
\end{equation}

The conditional density function $f_{0,t-1}(y_{1,t};\boldsymbol{\theta})$ in (\ref{eq:prodform}) is obtained from (\ref{eq:ytj_margdens}) by choosing $j=1$.  For $j=2,...,d$,  the conditional density functions  $f_{j-1,t-1}(y_{j,t};\boldsymbol{\theta})$ are obtained by substituting the equation (\ref{eq:ytj_margdens}) to the formula of conditional density function:
\begin{align}\label{eq:conddens_gstmvar}
\begin{aligned}
&f_{j-1,t-1}\left(y_{j,t};\boldsymbol{\theta}\right) = \frac{f_{t-1}(y_{t}^{(j)};\boldsymbol{\theta})}{f_{t-1}(y_{t}^{(j-1)};\boldsymbol{\theta})} = \\
&\frac{\sum_{m=1}^{M_1}\alpha_{m,t} n_j(y_{t}^{(j)};\mu_{m,t}^{(j)},\Omega_{m}^{(j)}) + \sum_{m=M_1+1}^{M}\alpha_{m,t} t_j(y_{t}^{(j)};\mu_{m,t}^{(j)},\Omega_{m,t}^{(j)}, \nu_m+dp)}{\sum_{n=1}^{M_1}\alpha_{n,t} n_{j-1}(y_{t}^{(j-1)};\mu_{n,t}^{(j-1)},\Omega_n^{(j-1)}) + \sum_{n=M_1+1}^{M}\alpha_{n,t} t_{j-1}(y_{t}^{(j-1)};\mu_{n,t}^{(j-1)},\Omega_n^{(j-1)},\nu_m + dp)}.
\end{aligned}
\end{align}

It follows from the properties of the conditional distributions of multivariate normal distribution that we may express the $j$-dimensional normal distributions as
\begin{equation}\label{eq:n1_nj-1}
n_j(y_{t}^{(j)};\mu_{m,t}^{(j)},\Omega_m^{(j)}) = n_1(y_{j,t};\mu_{m,t,j|j-1},\Omega_{m,j|j-1}) n_{j-1}(y_{t}^{(j-1)};\mu_{m,t}^{(j-1)},\Omega_m^{(j-1)}),
\end{equation}
where $\mu_{m,t,j|j-1}$ and $\Omega_{m,j|j-1}$ are the conditional mean and covariance matrix of $y_{j,t}$ conditional on $\sigma(\mathcal{A}_{j-1},\mathcal{F}_{t-1})$.  Likewise, it follows from the properties of the conditional distributions of multivariate $t$-distribution that we may express the $j$-dimensional $t$-distributions as
\begin{align}\label{eq:t1_tj-1}
\begin{aligned}
t_j(y_{t}^{(j)};\mu_{m,t}^{(j)},\Omega_{m,t}^{(j)},\nu_m + dp) =& t_1(y_{j,t};\mu_{m,t,j|j-1},\Omega_{m,t,j|j-1},\nu_m + dp + j  - 1) \\
&\times t_{j-1}(y_{t}^{(j-1)};\mu_{m,t}^{(j-1)},\Omega_{m,t}^{(j-1)},\nu_m + dp),
\end{aligned}
\end{align}
where $\mu_{m,t,j|j-1}$ and $\Omega_{m,t,j|j-1}$ are the conditional mean and covariance matrix of $y_{j,t}$ conditional on $\sigma(\mathcal{A}_{j-1},\mathcal{F}_{t-1})$.

By denoting
\begin{equation}\label{eq:beta_mtj_t}
\beta_{m,t,j} \equiv \frac{\alpha_{m,t}n_{j-1}(y_{t}^{(j-1)};\mu_{m,t}^{(j-1)},\Omega_{m}^{(j-1)})}{\sum_{n=1}^{M_1}\alpha_{n,t} n_{j-1}(y_{t}^{(j-1)};\mu_{n,t}^{(j-1)},\Omega_{n}^{(j-1)}) +\sum_{n=M_1+1}^{M}\alpha_{n,t} t_{j-1}(y_{t}^{(j-1)};\mu_{n,t}^{(j-1)},\Omega_{n,t}^{(j-1)},\nu_n + dp)}
\end{equation}
for $m=1,..,M_1$,  $j=2,...,d$,  and
\begin{equation}\label{eq:beta_mtj_t}
\beta_{m,t,j} \equiv \frac{\alpha_{m,t}t_{j-1}(y_{t}^{(j-1)};\mu_{m,t}^{(j-1)},\Omega_{m,t}^{(j-1)},\nu_m + dp)}{\sum_{n=1}^{M_1}\alpha_{n,t} n_{j-1}(y_{t}^{(j-1)};\mu_{n,t}^{(j-1)},\Omega_{n}^{(j-1)}) +\sum_{n=M_1+1}^{M}\alpha_{n,t} t_{j-1}(y_{t}^{(j-1)};\mu_{n,t}^{(j-1)},\Omega_{n,t}^{(j-1)},\nu_n + dp)}
\end{equation}
for $m=M_1+1,...,M$,  $j=2,...,d$, and using the expressions (\ref{eq:n1_nj-1}) and (\ref{eq:t1_tj-1}),  we can express the conditional density function (\ref{eq:conddens_gstmvar}) as
\begin{align}
\begin{aligned}
f_{j-1,t-1}\left(y_{j,t};\boldsymbol{\theta}\right)=&\sum_{m=1}^{M_1}\beta_{m,t,j}n_1(y_{j,t};\mu_{m,t,j|j-1},\Omega_{m,j|j-1})\\
& + \sum_{m=M_1+1}^{M}\beta_{m,t,j}t_1(y_{j,t};\mu_{m,t,j|j-1},\Omega_{m,t,j|j-1},\nu_m + dp + j - 1), \ \ j=2,..,d.
\end{aligned}
\end{align}

For $m=1,...,M_1$, the conditional means $\mu_{m,t,j|j-1}$ and covariance matrices $\Omega_{m,j|j-1}$ are as in (\ref{eq:mux_gaus}) and (\ref{eq:omegax_gaus}) when for each $j=2,...,d$ and $m=1,...,M$ we consider the partition $y_t^{(j)}=(y_t^{(j-1)},y_{j,t})$, $\mu_{m,t}^{(j)}=(\mu_{m,t}^{(j-1)},\mu_{j,m,t})$, and
\begin{equation}
\Omega_m^{(j)}=
\begin{bmatrix}
\Omega_m^{(j-1)} \quad\enspace & \Omega_{(j-1),j,m} \\
\Omega_{(j-1),j,m}' & \Omega_{m}(j,j)  \\
\end{bmatrix},
\end{equation}
where $\Omega_{m}(j,j)$ is the $jj$th elementh of $\Omega_m$ and $\Omega_{(j-1),j,m}$ $((j-1)\times 1)$  consists of the rows $1,...,j-1$ of the $j$th column of $\Omega_m$.  In particular, we have
\begin{align}
\mu_{m,t,j|j-1} &=\mu_{j,m,t} + \Omega_{(j-1),j,m}'(\Omega_m^{(j-1)})^{-1}(y_t^{(j-1)}-\mu_{m,t}^{(j-1)}),\label{eq:cond_mu_mtj_n}\\
\Omega_{m,j|j-1} &= \Omega_{m}(j,j) - \Omega_{(j-1),j,m}'(\Omega_m^{(j-1)})^{-1} \Omega_{(j-1),j,m}. \label{eq:cond_omega_mj_n}
\end{align}

For $m=M_1+1,..,M$, the conditional means $\mu_{m,t,j|j-1}$ and covariance matrices $\Omega_{m,t,j|j-1}$ are as in (\ref{eq:mux}) and (\ref{eq:omegax}) when for each $j=2,...,d$ and $m=1,...,M$ we consider the partition $y_t^{(j)}=(y_t^{(j-1)},y_{j,t})$, $\mu_{m,t}^{(j)}=(\mu_{m,t}^{(j-1)},\mu_{j,m,t})$, and
\begin{equation}
\Omega_{m,t}^{(j)}=
\begin{bmatrix}
\Omega_{m,t}^{(j-1)} \quad\enspace & \Omega_{(j-1),j,m,t} \\
\Omega_{(j-1),j,m,t}' & \Omega_{m,t}(j,j)  \\
\end{bmatrix},
\end{equation}
where $\Omega_{m,t}(j,j)$ is the $jj$th elementh of $\Omega_{m,t}$ and $\Omega_{(j-1),j,m,t}$ $((j-1)\times 1)$  consists of the rows $1,...,j-1$ of the $j$th column of $\Omega_{m,t}$.  In particular,  taking use of the relation $\Omega_{m,t}=\omega_{m,t}\Omega_m$ (where $\omega_{m,t}$ is scalar), we have
\begin{align}
\begin{aligned}
\mu_{m,t,j|j-1} &=\mu_{j,m,t} + \Omega_{(j-1),j,m,t}'(\Omega_{m,t}^{(j-1)})^{-1}(y_t^{(j-1)}-\mu_{m,t}^{(j-1)})\\
&=\mu_{j,m,t} + \Omega_{(j-1),j,m}'(\Omega_{m}^{(j-1)})^{-1}(y_t^{(j-1)}-\mu_{m,t}^{(j-1)}),
\end{aligned}\label{eq:cond_mu_mtj}
\end{align}
and
\begin{align}
\begin{aligned}
\Omega_{m,t,j|j-1} &= \frac{\nu_m + dp + (y_t^{(j-1)} - \mu_{m,t}^{(j - 1)})'(\Omega_{m,t}^{(j-1)})^{-1}(y_t^{(j-1)} - \mu_{m,t}^{(j - 1)})}{\nu_m + dp + j - 3}\tilde{\Omega}_{m,t,j|j-1}\\
&= \frac{\nu_m + dp + \omega_{m,t}^{-1}(y_t^{(j-1)} - \mu_{m,t}^{(j - 1)})'(\Omega_{m}^{(j-1)})^{-1}(y_t^{(j-1)} - \mu_{m,t}^{(j - 1)})}{\nu_m + dp + j - 3}\tilde{\Omega}_{m,t,j|j-1},
\end{aligned}\label{eq:cond_omega_mtj}
\end{align}
where
\begin{align}
\begin{aligned}
\tilde{\Omega}_{m,t,j|j-1} &\equiv \Omega_{m,t}(j,j) - \Omega_{(j-1),j,m,t}'(\Omega_{m,t}^{(j-1)})^{-1} \Omega_{(j-1),j,m,t}\\
&=\omega_{m,t}(\Omega_{m}(j,j) - \Omega_{(j-1),j,m}'(\Omega_{m}^{(j-1)})^{-1} \Omega_{(j-1),j,m}).
\end{aligned}
\end{align}

It then remains to find expressions for the conditional distribution functions $F_{j-1,t-1}(y_{j,t};\boldsymbol{\theta})$, $j=1,...,d$, in (\ref{eq:qrdef}).  For notational convenience, we write
\begin{align}
\begin{aligned}
f_{j-1,t-1}(y_{j,t}\boldsymbol{\theta}) =& \sum_{m=1}^{M_1}\beta_{m,t,j}n_1(y_{j,t};\mu_{m,t,j|j-1},\Omega_{m,j|j-1}) \\
&+ \sum_{m=M_1 + 1}^{M}\beta_{m,t,j}t_1(y_{j,t};\mu_{m,t,j|j-1},\Omega_{m,t,j|j-1},\nu_m + dp + j - 1)
\end{aligned}
\end{align}
for all $j=1,...,d$ by defining $\beta_{m,t,1}\equiv \alpha_{m,t}$,  $\mu_{m,t,1|0}\equiv \mu_{m,t}^{(1)}$,  $\Omega_{m,1|0}\equiv \Omega_{m}^{(1)}$, and $\Omega_{m,t,1|0}\equiv \Omega_{m,t}^{(1)}$.  For $j=2,...,d$,  these quantities are defined in (\ref{eq:beta_mtj_t}), (\ref{eq:cond_mu_mtj_n}), (\ref{eq:cond_omega_mj_n}), (\ref{eq:cond_mu_mtj}), and (\ref{eq:cond_omega_mtj}). Then,
\begin{align}
\begin{aligned}
F_{j-1,t-1}(y_{j,t};\boldsymbol{\theta})=&\sum_{m=1}^{M_1}\beta_{m,t,j}\int_{-\infty}^{y_{j,t}}n_1(u;\mu_{m,t,j|j-1},\Omega_{m,j|j-1})du \\
&+\sum_{m=M_1+1}^{M}\beta_{m,t,j}\int_{-\infty}^{y_{j,t}}t_1(u;\mu_{m,t,j|j-1},\Omega_{m,t,j|j-1},\nu_m + dp + j - 1)du,
\end{aligned}
\end{align}
where we seek to solve the integrals inside the sums.

Regarding the first sum,  for $m=1,...,M_1$,  it is easy to see that the integrals can be expressed using the stardard normal distribution function $\Phi(\cdot)$ as
\begin{align}
\int_{-\infty}^{y_{j,t}}n_1(u;\mu_{m,t,j|j-1},\Omega_{m,j|j-1})du &= \Phi\left(\frac{y_{j,t}-\mu_{m,t,j|j-1}}{\sqrt{\Omega_{m,j|j-1}}}\right).
\end{align}

Next, consider the second sum,  $m=M_1+1,...,M$. By taking use of the symmetry of the $t$-distribution about its mean, we obtain
\begin{align}\label{eq:integral1}
\begin{aligned}
&\int_{-\infty}^{y_{j,t}}t_1(u;\mu_{m,t,j|j-1},\Omega_{m,t,j|j-1},\nu_m + dp + j - 1)du \\
&= \frac{1}{2} + \int_{\mu_{m,t,j|j-1}}^{y_{j,t}}t_1(u;\mu_{m,t,j|j-1},\Omega_{m,t,j|j-1},\nu_m + dp + j - 1)du.
\end{aligned}
\end{align}
By applying the change of variables $\tilde{u}_{m,t,j} = u - \mu_{m,t,j|j-1}$ in the integral, the RHS of (\ref{eq:integral1}) can be expressed as
\begin{equation}\label{eq:integral2}
\frac{1}{2} + \frac{\Gamma\left(\frac{\nu_m + dp + j}{2}\right)}{\sqrt{\pi (\nu_m + dp + j - 3)}\Gamma\left(\frac{\nu_m + dp + j - 1}{2} \right)}\Omega_{m,t,j|j-1}^{-1/2}\int_{0}^{\tilde{y}_{m,t,j}}\left(1 + \frac{\tilde{u}_{m,t,j}^2}{a_{m,t,j}} \right)^{-b_{m,j}}d\tilde{u}_{m,t,j},
\end{equation}
where $\tilde{y}_{m,t,j} \equiv y_{j,t} - \mu_{m,t,j|j-1}$,  $a_{m,t,j} \equiv (\nu_m + dp + j - 3)\Omega_{m,t,j|j-1}$, and $b_{m,j}\equiv (\nu_m + dp + j)/2$.

Then, by applying the change of variables $z_{m,t,j} =  \tilde{u}_{m,t,j}^2/\tilde{y}_{m,t,j}$,  we can express the integral in (\ref{eq:integral2}) as
\begin{equation}\label{eq:integral3}
\int_{0}^{\tilde{y}_{m,t,j}}\left(1 + \frac{\tilde{u}_{m,t,j}^2}{a_{m,t,j}} \right)^{-b_{m,j}}d\tilde{u}_{m,t,j} = \frac{1}{2}\int_{0}^{\tilde{y}_{m,t,j}}\left(\frac{\tilde{y}_{m,t,j}}{z_{m,t,j}}  \right)^{1/2}\left(1 + \frac{z_{m,t,j}\tilde{y}_{m,t,j}}{a_{m,t,j}} \right)^{-b_{m,j}}dz_{m,t,j}.
\end{equation}
By applying the third change of variables $x_{m,t,j}=z_{m,t,j}/\tilde{y}_{m,t,j}$ and using the properties of the gamma function,  the RHS of (\ref{eq:integral3}) can be expressed as
\begin{equation}
\frac{\tilde{y}_{m,t,j}}{2}\int_0^1 x_{m,t,j}^{-1/2}\left(1 - x_{m,t,j}\left(-\frac{\tilde{y}_{m,t,j}^2}{a_{m,t,j}}\right)  \right)^{-b_{m,j}}dx_{m,t,j} = \tilde{y}_{m,t,j} \times {}_2\text{F}_1\left(\frac{1}{2}, b_{m,j}, \frac{3}{2}; -\frac{\tilde{y}_{m,t,j}^2}{a_{m,t,j}} \right),
\end{equation}
where the hypergeometric function is defined as \citep[Section 1.3.1]{Aomoto+Kita:2011}
\begin{equation}
{}_2F_1(a,b,c;x)=\frac{\Gamma(c)}{\Gamma(a)\Gamma(c-a)}\int_0^1 s^{a-1}(1-s)^{c-a-1}(1-sx)^{-b}ds,
\end{equation}
when $|x|<1$, $a>0$, and $c-a>0$ (when $a,c\in\mathbb{R}$).

Using the above result, we have
\begin{align}\label{eq:stmvar_qr_closedform}
\begin{aligned}
&\int_{-\infty}^{y_{j,t}}t_1(u;\mu_{m,t,j|j-1},\Omega_{m,t,j|j-1},\nu_m + dp + j - 1)du \\
&= \frac{1}{2} + \frac{\Gamma\left(\frac{\nu_m + dp + j}{2}\right)}{\sqrt{\pi (\nu_m + dp + j - 3)}\Gamma\left(\frac{\nu_m + dp + j - 1}{2} \right)}\Omega_{m,t,j|j-1}^{-1/2}\tilde{y}_{m,t,j} \times {}_2\text{F}_1\left(\frac{1}{2}, b_{m,j}, \frac{3}{2}; -\frac{\tilde{y}_{m,t,j}^2}{a_{m,t,j}} \right)
\end{aligned}
\end{align}
whenever $\left|-\frac{\tilde{y}_{m,t,j}^2}{a_{m,t,j}}\right|<1$.  That is,  the closed form expression (\ref{eq:stmvar_qr_closedform}) exists when
\begin{equation}
|y_{j,t} - \mu_{m,t,j|j-1}| < \sqrt{(v_m + dp + j - 3)\Omega_{m,t,j,|j-1}}.
\end{equation}
If this condition does not hold, the quantile residual are obtained by numerically integrating the conditional density function $t_1(u;\mu_{m,t,j|j-1},\Omega_{m,t,j|j-1},\nu_m + dp + j - 1)$. For the hypergeometric function, \pkg{gmvarkit} uses the package \pkg{gsl} \citep{gsl}.


\section{Monte Carlo algorithm}\label{sec:montecarlo}
We present a Monte Carlo algorithm that produces point estimates and with random initial value $\boldsymbol{y}_{t-1}=(y_{t-1},...,y_{t-p})$ confidence intervals for the generalized impulse response function defined in (\ref{eq:girf}). Our algorithm is adapted from \citet[pp. 135-136]{Koop+Pesaran+Potter:1996} and \citet[pp. 601-602]{Kilian+Lutkepohl:2017}. We assume that the history $\boldsymbol{y}_{t-1}$ follows a known distribution $G$, which may be such that it produces a single outcome with probability one (corresponding to a fixed $\boldsymbol{y}_{t-1}$), or it can be the stationary distribution of the process or of a specific regime. In the following, $y_{t+h}^{(i)}(\delta_j,\boldsymbol{y}_{t-1})$ denotes a realization of the process at time $t+h$ conditional on the structural shock of magnitude $\delta_j$ in the $j$th element of $e_t$ hitting the system at time $t$ and on the $p$ observations $\boldsymbol{y}_{t-1}=(y_{t-1},...,y_{t-p})$ preceding the time $t$, whereas $y_{t+h}^{(i)}(\boldsymbol{y}_{t-1})$ denotes an alternative realization conditional on the history $\boldsymbol{y}_{t-1}$ only.

The algorithm proceeds with the following steps.
\begin{enumerate}\addtocounter{enumi}{-1}
\item Decide the horizon $H$, the numbers of repetitions $R_1$ and $R_2$, and the magnitude $\delta_j$ for the $j$th structural shock that is of interest.

\item Draw an initial value $\boldsymbol{y}_{t-1}$ from $G$.\label{step1}

\item Draw $H+1$ independent realizations of a shock $\varepsilon_t$ from $N(0,I_d)$. If the models contains Student's $t$ regime, the Gaussian shocks are used obtain shocks from the appropriate Student's $t$ distributions (in this case, $H+1$ independent realizations of shocks from $\chi^2_{\nu_m + dp}$-distributions are also drawn for the $t$-regimes). Also, draw an initial regime $m\in \lbrace 1,...,M \rbrace$ according to the probabilities given by the mixing weights $\alpha_{1,t},...,\alpha_{M,t}$ and compute the reduced form shock $u_t$. If identification by heteroskedasticity is used, $u_t=W\Lambda_m^{1/2}\varepsilon_t$, where $\Lambda_1=I_d$. If the shocks are, instead, identified recursively, $u_t=L_t\varepsilon_t$, where $L_t$ is the lower triangular matrix obtained from the Cholesky decomposition $\Omega_t = L_tL_t'$ and $\Omega_t$ is the conditional covariance matrix of the process. Then, compute the structural shock $e_t = B_t^{-1}u_t$ and impose the size $\delta_j$ on its $j$th element to obtain $e_t^*$. Finally, calculate the modified reduced form shock $u_t^*=B_te_t^*$.\label{step2} %\footnote{The independent standard normal shocks $\varepsilon_t$ are introduced here to control random variation across the two sample paths $y_{t+n}^{(i)}(\delta_j,\boldsymbol{y}_{t-1})$ and $y_{t+n}^{(i)}(\boldsymbol{y}_{t-1})$.}

\item Use the modified reduced form shock $u_t^*$ and the rest $H$ standard normal shocks $\varepsilon_t$ obtained from Step~\ref{step2} to compute realizations $y_{t+h}^{(i)}(\delta_j,\boldsymbol{y}_{t-1})$ for $n=0,1,...,H$, iterating forward so that in each iteration the regime $m$ that generates the observation is first drawn according to the probabilities given by the mixing weights. At $h=0$, the initial regime and the modified reduced form shock $u_t^*$ calculated from the structural shock in Step~\ref{step2} is used. From $h=1$ onwards, the $h+1$th standard normal shock $\varepsilon_t$ is used to calculate the reduced form shock $u_{t+h}=W\Lambda_m^{1/2}\varepsilon_{t+h}$, where $\Lambda_1=I_d$ and $m$ is the selected regime (note that only the reduced form shocks are of interest with $h>0$, so the same formula can be used for recursively identified shocks here).

\item  Use the reduced form shock $u_t$ and the rest $H$ the standard normal shocks $\varepsilon_t$ obtained from  Step~\ref{step2} to compute realizations $y_{t+h}^{(i)}(\boldsymbol{y}_{t-1})$ for $h=0,1,...,H$, so that the reduced form shock $u_t$ (calculated in Step~\ref{step2}) is used to compute the time $h=0$ realization. Otherwise proceed similarly to the previous step.

\item Calculate $y_{t+h}^{(i)}(\delta_j,\boldsymbol{y}_{t-1}) - y_{t+h}^{(i)}(\boldsymbol{y}_{t-1})$.\label{step5}

\item Repeat Steps~\ref{step2}-\ref{step5} $R_1$ times and calculate the sample mean of $y_{t+h}^{(i)}(\delta_j,\boldsymbol{y}_{t-1}) - y_{t+n}^{(i)}(\boldsymbol{y}_{t-1})$ for $h=0,1,...,H$ to obtain an estimate of the GIRF$(h,\delta_j,\boldsymbol{y}_{t-1})$.\label{step6}

\item Repeat Steps~\ref{step1}-\ref{step6} $R_2$ times to obtain estimates of GIRF$(h,\delta_j,\boldsymbol{y}_{t-1})$ with different starting values $\boldsymbol{y}_{t-1}$ generated from the distribution $G$. Then, take the sample mean and sample quantiles over the estimates to obtain point estimate and confidence intervals for the GIRF with random initial value.\label{step7}
\end{enumerate}
Notice that if a fixed initial value $\boldsymbol{y}_{t-1}$ is used, Step~\ref{step7} is redundant.


\end{appendix}

\end{document}
